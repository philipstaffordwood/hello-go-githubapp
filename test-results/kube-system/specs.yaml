apiVersion: v1
items:
- items:
  - apiVersion: v1
    data:
      calico_backend: bird
      cluster_cidr: 100.200.0.0/16
      cluster_type: k8s,bgp
      cni_network_config: |-
        {
          "name": "k8s-pod-network",
          "cniVersion": "0.3.1",
          "plugins": [
            {
              "type": "calico",
              "log_level": "info",
              "datastore_type": "kubernetes",
              "nodename": "__KUBERNETES_NODE_NAME__",
              "mtu": __CNI_MTU__,
              "ipam": {
                  "type": "calico-ipam"
              },
              "policy": {
                  "type": "k8s"
              },
              "kubernetes": {
                  "kubeconfig": "__KUBECONFIG_FILEPATH__"
              }
            },
            {
              "type": "portmap",
              "snat": true,
              "capabilities": {"portMappings": true}
            }
          ]
        }
      felix_log_level: info
      ipv4pool_ipip: Never
      typha_service_name: none
      veth_mtu: "1440"
    kind: ConfigMap
    metadata:
      creationTimestamp: "2020-04-27T20:34:24Z"
      name: calico-config
      namespace: kube-system
      resourceVersion: "312"
      selfLink: /api/v1/namespaces/kube-system/configmaps/calico-config
      uid: e683d4fe-48a6-45ac-b9c9-a29b47835581
  - apiVersion: v1
    kind: ConfigMap
    metadata:
      annotations:
        control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"cert-manager-cainjector-5cc6f8695f-6zdn6_e8c9bb78-2759-4758-b2f0-a76b29259998","leaseDurationSeconds":15,"acquireTime":"2020-04-27T20:34:50Z","renewTime":"2020-04-27T20:46:09Z","leaderTransitions":0}'
      creationTimestamp: "2020-04-27T20:34:50Z"
      name: cert-manager-cainjector-leader-election
      namespace: kube-system
      resourceVersion: "7189"
      selfLink: /api/v1/namespaces/kube-system/configmaps/cert-manager-cainjector-leader-election
      uid: 2b34bf8d-a907-459c-aad8-c6f20c30a8d1
  - apiVersion: v1
    kind: ConfigMap
    metadata:
      annotations:
        control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"cert-manager-cainjector-5cc6f8695f-6zdn6_7bc87e86-da04-458b-a76c-a3f99c3ac47b","leaseDurationSeconds":15,"acquireTime":"2020-04-27T20:34:50Z","renewTime":"2020-04-27T20:46:09Z","leaderTransitions":0}'
      creationTimestamp: "2020-04-27T20:34:50Z"
      name: cert-manager-cainjector-leader-election-core
      namespace: kube-system
      resourceVersion: "7190"
      selfLink: /api/v1/namespaces/kube-system/configmaps/cert-manager-cainjector-leader-election-core
      uid: 8a212024-5d9c-4ce9-8f49-f9898200f530
  - apiVersion: v1
    kind: ConfigMap
    metadata:
      annotations:
        control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"cert-manager-67b8bfbfb4-9wsv9-external-cert-manager-controller","leaseDurationSeconds":60,"acquireTime":"2020-04-27T20:34:53Z","renewTime":"2020-04-27T20:46:08Z","leaderTransitions":0}'
      creationTimestamp: "2020-04-27T20:34:53Z"
      name: cert-manager-controller
      namespace: kube-system
      resourceVersion: "7185"
      selfLink: /api/v1/namespaces/kube-system/configmaps/cert-manager-controller
      uid: bc5ce18d-e80f-43cd-a311-eb6d579fc98f
  - apiVersion: v1
    data:
      Corefile: |
        .:53 {
            errors
            health
            kubernetes cluster.local in-addr.arpa ip6.arpa {
               pods insecure
               upstream
               fallthrough in-addr.arpa ip6.arpa
               ttl 30
            }
            prometheus :9153
            forward . /etc/resolv.conf
            cache 30
            loop
            reload
            loadbalance
        }
    kind: ConfigMap
    metadata:
      creationTimestamp: "2020-04-27T20:34:17Z"
      name: coredns
      namespace: kube-system
      resourceVersion: "167"
      selfLink: /api/v1/namespaces/kube-system/configmaps/coredns
      uid: 561d02f3-406c-4fa6-8e9f-a9d1070cf488
  - apiVersion: v1
    data:
      client-ca-file: |
        -----BEGIN CERTIFICATE-----
        MIICyDCCAbCgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl
        cm5ldGVzMB4XDTIwMDQyNzIwMzM1OFoXDTMwMDQyNTIwMzM1OFowFTETMBEGA1UE
        AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALGS
        Xz36DKS6iGdx8yzuRL38z55kLmNzsW/im16cvf0M6BsRTQ2gJrl8VdfxR2lCT4qn
        +o9fT2RmJjABRqzWg4f+vHkYY57eFiiiiVC6pSUGJZIprYZx8AwepWh/X2PJWbyE
        L1iE8Tj4wm0U2n6+pSM2mIQ9VOrtPP+qG9o7kB2NCm/O9HjUnbt2qEPq6x1lFsuy
        rfgW/TwqTLNJrGD9L1qlbzpTKRwIQR9W6rIDm5KGVl6fvjgKt2yCdGEeCEPiUxwK
        Y24qXErsGqyR4JFfvt9o/HYZca6jjDWsE/w12flzw92CAGUI2CdiJusdWsWT6xmd
        VK8SCZPClonu4mCyiPkCAwEAAaMjMCEwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB
        /wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBACfZ4MoK2jeaUKK6baUVJG4WTbp0
        AOHfOk2c6RttYV1Wrh2Cw3io08/HmN0VM3Xr62Ct5ZbQq10rM9M4NsAK4u7MHo+o
        DqUNEUBSKy7IO0+C3/kwzZSKYPX6ZmFkfrUUJAyanWtTR7x+BCB50T245HL9Eq2H
        gT/ZCGz5gP1HmMMPvPNV5/Z66QIUZKvzIIgOUNi0T0uR8Lwimxn4rRX7liFIuo93
        kVLy5QCBLexxcuwitU0k+TaV+CpKAFrVAXliGIp8/6O8qfDUD1vYnCSiL+Ee3gR/
        1P9wWKS7VFhNuH2DRYNVagFqU54dEfSHEDSdUVyugfs5FaI0frSpM/Wlp6k=
        -----END CERTIFICATE-----
      requestheader-allowed-names: '["front-proxy-client"]'
      requestheader-client-ca-file: |
        -----BEGIN CERTIFICATE-----
        MIIC0DCCAbigAwIBAgIBADANBgkqhkiG9w0BAQsFADAZMRcwFQYDVQQDEw5mcm9u
        dC1wcm94eS1jYTAeFw0yMDA0MjcyMDMzNTlaFw0zMDA0MjUyMDMzNTlaMBkxFzAV
        BgNVBAMTDmZyb250LXByb3h5LWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIB
        CgKCAQEAxtCXbnsBs88typAwxkuXicISJ1mSH80lwbsYP4JR5yizVoWWOHhzHKLp
        qwJuzMmg+QYNQrFvXmQAMkClsT/ZPLj3BRhmE3swHb8AEKVpYtqeQ4Swqi6QkrYr
        jWFQ2BCUlCbmfum/fLXNNIzv23sSQhmuToFXfiRClxZ7wcHHWdxznN9AtshjQE4D
        62OXSrzZTTAPEvycZUNVNm1AsaXxU4ZtzSB92R1I/LsTDsGuX0PlVDpggLLnTE2u
        RvswXNQ+VSDrLOknayB4h+XmqO7XyotgxqsO+BgpTtacsdpJenQFcf0SlQvtCOOo
        i8UXSB7VKkgHuVfBxCE6CiYRGOg2BwIDAQABoyMwITAOBgNVHQ8BAf8EBAMCAqQw
        DwYDVR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAjpUKFLU1rZfRTOHI
        nKwMMTHE1B0pDIwohsmlGuRHy+IoJ32Micc1XTLtaAkLfBgkxSc08FGFNT5lDL4j
        6jmcSYgjh4D//aWHNS6d89g85KmrNgaCNeLc/jUbNDoMt1tgqENHAg8bR2Sc/l4Y
        C1hFtyKpJiC72zoFfZHlkHAaBE9HD8uQOe5Qf5UlXeS9i3VbQ62LCHu+XpTMF3fc
        2Jy8tu1te1jnhT1SYeqbFbWt1YmZ54fsOe1jIewA0TjbZULbP4xAxLOEso08wprN
        I94ftywpvAXLr+eYBqNL1CQzsx/ZF/osdHMOdFDKGv8GV1/uUNeUxnG4JHOlMnUv
        amI70A==
        -----END CERTIFICATE-----
      requestheader-extra-headers-prefix: '["X-Remote-Extra-"]'
      requestheader-group-headers: '["X-Remote-Group"]'
      requestheader-username-headers: '["X-Remote-User"]'
    kind: ConfigMap
    metadata:
      creationTimestamp: "2020-04-27T20:34:15Z"
      name: extension-apiserver-authentication
      namespace: kube-system
      resourceVersion: "43"
      selfLink: /api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication
      uid: 574ae5b7-da14-4350-a0da-3deabaf04919
  - apiVersion: v1
    data:
      fluent.conf: |
        <match systemd.** kube.kube-system.** k8s.** docker* kube.harbor.** kube.pgo** kube.quack.** kube.opa.** kube.cert-manager.** kube.dex.** kube.monitoring.** kube.velero.** kube.local-path-storage.** kube.elastic-system.** >
          @include /etc/fluentd/config.d/elasticsearch.conf
        </match>
    kind: ConfigMap
    metadata:
      creationTimestamp: "2020-04-27T20:38:36Z"
      name: fluentd-config
      namespace: kube-system
      resourceVersion: "2578"
      selfLink: /api/v1/namespaces/kube-system/configmaps/fluentd-config
      uid: 9ed1dc72-ec34-4026-be5a-feb9e522a5c3
  - apiVersion: v1
    data:
      config.conf: |-
        apiVersion: kubeproxy.config.k8s.io/v1alpha1
        bindAddress: 0.0.0.0
        clientConnection:
          acceptContentTypes: ""
          burst: 10
          contentType: application/vnd.kubernetes.protobuf
          kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
          qps: 5
        clusterCIDR: 10.244.0.0/16
        configSyncPeriod: 15m0s
        conntrack:
          maxPerCore: 32768
          min: 131072
          tcpCloseWaitTimeout: 1h0m0s
          tcpEstablishedTimeout: 24h0m0s
        enableProfiling: false
        healthzBindAddress: 0.0.0.0:10256
        hostnameOverride: ""
        iptables:
          masqueradeAll: false
          masqueradeBit: 14
          minSyncPeriod: 0s
          syncPeriod: 30s
        ipvs:
          excludeCIDRs: null
          minSyncPeriod: 0s
          scheduler: ""
          strictARP: false
          syncPeriod: 30s
        kind: KubeProxyConfiguration
        metricsBindAddress: 127.0.0.1:10249
        mode: ""
        nodePortAddresses: null
        oomScoreAdj: -999
        portRange: ""
        resourceContainer: /kube-proxy
        udpIdleTimeout: 250ms
        winkernel:
          enableDSR: false
          networkName: ""
          sourceVip: ""
      kubeconfig.conf: |-
        apiVersion: v1
        kind: Config
        clusters:
        - cluster:
            certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            server: https://172.17.0.2:6443
          name: default
        contexts:
        - context:
            cluster: default
            namespace: default
            user: default
          name: default
        current-context: default
        users:
        - name: default
          user:
            tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    kind: ConfigMap
    metadata:
      creationTimestamp: "2020-04-27T20:34:17Z"
      labels:
        app: kube-proxy
      name: kube-proxy
      namespace: kube-system
      resourceVersion: "177"
      selfLink: /api/v1/namespaces/kube-system/configmaps/kube-proxy
      uid: 3993b969-47b4-4902-b6fd-dac549bea989
  - apiVersion: v1
    data:
      ClusterConfiguration: |
        apiServer:
          certSANs:
          - localhost
          - 127.0.0.1
          extraArgs:
            audit-log-format: legacy
            audit-log-maxage: "2"
            audit-log-maxbackup: "3"
            audit-log-maxsize: "1024"
            audit-log-path: /var/log/audit/cluster-audit.log
            audit-policy-file: /etc/kubernetes/policies/audit-policy.yaml
            authorization-mode: Node,RBAC
            oidc-ca-file: /etc/ssl/oidc/ingress-ca.pem
            oidc-client-id: kubernetes
            oidc-groups-claim: groups
            oidc-issuer-url: https://dex.127.0.0.1.nip.io
            oidc-username-claim: email
          extraVolumes:
          - hostPath: /etc/kubernetes/policies/audit-policy.yaml
            mountPath: /etc/kubernetes/policies/audit-policy.yaml
            name: auditpolicy
            pathType: File
            readOnly: true
          - hostPath: /etc/flanksource/ingress-ca/ingress-ca.crt
            mountPath: /etc/ssl/oidc/ingress-ca.pem
            name: oidc-certificates
            pathType: File
            readOnly: true
          timeoutForControlPlane: 4m0s
        apiVersion: kubeadm.k8s.io/v1beta2
        certificatesDir: /etc/kubernetes/pki
        clusterName: test
        controlPlaneEndpoint: 172.17.0.2:6443
        controllerManager:
          extraArgs:
            enable-hostpath-provisioner: "true"
        dns:
          type: CoreDNS
        etcd:
          local:
            dataDir: /var/lib/etcd
            extraArgs:
              listen-metrics-urls: http://0.0.0.0:2381
        imageRepository: k8s.gcr.io
        kind: ClusterConfiguration
        kubernetesVersion: v1.15.7
        networking:
          dnsDomain: cluster.local
          podSubnet: 10.244.0.0/16
          serviceSubnet: 10.96.0.0/12
        scheduler: {}
      ClusterStatus: |
        apiEndpoints:
          kind-control-plane:
            advertiseAddress: 172.17.0.2
            bindPort: 6443
        apiVersion: kubeadm.k8s.io/v1beta2
        kind: ClusterStatus
    kind: ConfigMap
    metadata:
      creationTimestamp: "2020-04-27T20:34:16Z"
      name: kubeadm-config
      namespace: kube-system
      resourceVersion: "152"
      selfLink: /api/v1/namespaces/kube-system/configmaps/kubeadm-config
      uid: c8b2d914-e2ce-4228-9bc4-70e6df885896
  - apiVersion: v1
    data:
      kubelet: |
        address: 0.0.0.0
        apiVersion: kubelet.config.k8s.io/v1beta1
        authentication:
          anonymous:
            enabled: false
          webhook:
            cacheTTL: 2m0s
            enabled: true
          x509:
            clientCAFile: /etc/kubernetes/pki/ca.crt
        authorization:
          mode: Webhook
          webhook:
            cacheAuthorizedTTL: 5m0s
            cacheUnauthorizedTTL: 30s
        cgroupDriver: cgroupfs
        cgroupsPerQOS: true
        clusterDNS:
        - 10.96.0.10
        clusterDomain: cluster.local
        configMapAndSecretChangeDetectionStrategy: Watch
        containerLogMaxFiles: 5
        containerLogMaxSize: 10Mi
        contentType: application/vnd.kubernetes.protobuf
        cpuCFSQuota: true
        cpuCFSQuotaPeriod: 100ms
        cpuManagerPolicy: none
        cpuManagerReconcilePeriod: 10s
        enableControllerAttachDetach: true
        enableDebuggingHandlers: true
        enforceNodeAllocatable:
        - pods
        eventBurst: 10
        eventRecordQPS: 5
        evictionHard:
          imagefs.available: 0%
          nodefs.available: 0%
          nodefs.inodesFree: 0%
        evictionPressureTransitionPeriod: 5m0s
        failSwapOn: true
        fileCheckFrequency: 20s
        hairpinMode: promiscuous-bridge
        healthzBindAddress: 127.0.0.1
        healthzPort: 10248
        httpCheckFrequency: 20s
        imageGCHighThresholdPercent: 100
        imageGCLowThresholdPercent: 80
        imageMinimumGCAge: 2m0s
        iptablesDropBit: 15
        iptablesMasqueradeBit: 14
        kind: KubeletConfiguration
        kubeAPIBurst: 10
        kubeAPIQPS: 5
        makeIPTablesUtilChains: true
        maxOpenFiles: 1000000
        maxPods: 110
        nodeLeaseDurationSeconds: 40
        nodeStatusReportFrequency: 1m0s
        nodeStatusUpdateFrequency: 10s
        oomScoreAdj: -999
        podPidsLimit: -1
        port: 10250
        registryBurst: 10
        registryPullQPS: 5
        resolvConf: /etc/resolv.conf
        rotateCertificates: true
        runtimeRequestTimeout: 2m0s
        serializeImagePulls: true
        staticPodPath: /etc/kubernetes/manifests
        streamingConnectionIdleTimeout: 4h0m0s
        syncFrequency: 1m0s
        volumeStatsAggPeriod: 1m0s
    kind: ConfigMap
    metadata:
      creationTimestamp: "2020-04-27T20:34:16Z"
      name: kubelet-config-1.15
      namespace: kube-system
      resourceVersion: "155"
      selfLink: /api/v1/namespaces/kube-system/configmaps/kubelet-config-1.15
      uid: 8d2b4f78-bd1c-4e6a-8d55-0dbee568b8a8
  - apiVersion: v1
    kind: ConfigMap
    metadata:
      creationTimestamp: "2020-04-27T20:35:53Z"
      name: namespace-configuration-operator-lock
      namespace: kube-system
      ownerReferences:
      - apiVersion: v1
        kind: Pod
        name: namespace-configuration-operator-6f748b789c-p2v6j
        uid: 1e147dfe-dce3-406e-89eb-79fec6c40830
      resourceVersion: "1262"
      selfLink: /api/v1/namespaces/kube-system/configmaps/namespace-configuration-operator-lock
      uid: 9a04ab38-6bc0-407d-af1d-d578826bd51f
  - apiVersion: v1
    data:
      Corefile: |
        cluster.local:53 {
            errors
            cache {
                success 9984 30
                denial 9984 5
            }
            reload
            loop
            bind 169.254.20.10 10.96.0.10
            forward . __PILLAR__CLUSTER__DNS__ {
                force_tcp
            }
            prometheus :9253
            health 169.254.20.10:8080
        }
        in-addr.arpa:53 {
            errors
            cache 30
            reload
            loop
            bind 169.254.20.10 10.96.0.10
            forward . __PILLAR__CLUSTER__DNS__ {
                force_tcp
            }
            prometheus :9253
        }
        ip6.arpa:53 {
            errors
            cache 30
            reload
            loop
            bind 169.254.20.10 10.96.0.10
            forward . __PILLAR__CLUSTER__DNS__ {
                force_tcp
            }
            prometheus :9253
        }
        .:53 {
            errors
            cache 30
            reload
            loop
            bind 169.254.20.10 10.96.0.10
            forward . __PILLAR__UPSTREAM__SERVERS__ {
                force_tcp
            }
            prometheus :9253
        }
    kind: ConfigMap
    metadata:
      creationTimestamp: "2020-04-27T20:35:28Z"
      labels:
        addonmanager.kubernetes.io/mode: Reconcile
      name: node-local-dns
      namespace: kube-system
      resourceVersion: "816"
      selfLink: /api/v1/namespaces/kube-system/configmaps/node-local-dns
      uid: 7f436229-ebe5-461a-ac92-e3b90a21a65e
  metadata: {}
- items:
  - apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        cni.projectcalico.org/podIP: 100.200.82.2/32
        scheduler.alpha.kubernetes.io/critical-pod: ""
      creationTimestamp: "2020-04-27T20:34:35Z"
      generateName: calico-kube-controllers-65b8787765-
      labels:
        k8s-app: calico-kube-controllers
        pod-template-hash: 65b8787765
      name: calico-kube-controllers-65b8787765-q8mpm
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: ReplicaSet
        name: calico-kube-controllers-65b8787765
        uid: 8492a565-f513-4f1a-aace-c607bc964343
      resourceVersion: "668"
      selfLink: /api/v1/namespaces/kube-system/pods/calico-kube-controllers-65b8787765-q8mpm
      uid: 46e01746-5d5f-4be6-87cd-7a74aede9bc2
    spec:
      containers:
      - env:
        - name: ENABLED_CONTROLLERS
          value: node
        - name: DATASTORE_TYPE
          value: kubernetes
        image: calico/kube-controllers:v3.8.2
        imagePullPolicy: IfNotPresent
        name: calico-kube-controllers
        readinessProbe:
          exec:
            command:
            - /usr/bin/check-status
            - -r
          failureThreshold: 3
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: calico-kube-controllers-token-9nhnx
          readOnly: true
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      nodeName: kind-control-plane
      nodeSelector:
        beta.kubernetes.io/os: linux
      priority: 2000000000
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: calico-kube-controllers
      serviceAccountName: calico-kube-controllers
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 300
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 300
      volumes:
      - name: calico-kube-controllers-token-9nhnx
        secret:
          defaultMode: 420
          secretName: calico-kube-controllers-token-9nhnx
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:47Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:54Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:54Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:47Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://d0834b43d9e1d9a4f3be46751230c833d4d002f1f3ada7105d5588410ad8ecd5
        image: docker.io/calico/kube-controllers:v3.8.2
        imageID: docker.io/calico/kube-controllers@sha256:afc0e28b569059abc6f5e199048c2b4f1d520dece9b16e4ddc3e4edb477c72ed
        lastState: {}
        name: calico-kube-controllers
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:34:51Z"
      hostIP: 172.17.0.2
      phase: Running
      podIP: 100.200.82.2
      qosClass: BestEffort
      startTime: "2020-04-27T20:34:47Z"
  - apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
      creationTimestamp: "2020-04-27T20:34:35Z"
      generateName: calico-node-
      labels:
        controller-revision-hash: fc88f6b8b
        k8s-app: calico-node
        pod-template-generation: "1"
      name: calico-node-lvdsp
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: DaemonSet
        name: calico-node
        uid: 7bf1cc4a-c741-466f-9ab4-ed1d4fb6d471
      resourceVersion: "676"
      selfLink: /api/v1/namespaces/kube-system/pods/calico-node-lvdsp
      uid: efbfe231-796c-4ac4-8cbd-d31dd7b344a2
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchFields:
              - key: metadata.name
                operator: In
                values:
                - kind-control-plane
      containers:
      - env:
        - name: FELIX_IGNORELOOSERPF
          value: "true"
        - name: DATASTORE_TYPE
          value: kubernetes
        - name: WAIT_FOR_DATASTORE
          value: "true"
        - name: NODENAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: CALICO_NETWORKING_BACKEND
          valueFrom:
            configMapKeyRef:
              key: calico_backend
              name: calico-config
        - name: CLUSTER_TYPE
          valueFrom:
            configMapKeyRef:
              key: cluster_type
              name: calico-config
        - name: IP
          value: autodetect
        - name: CALICO_IPV4POOL_IPIP
          valueFrom:
            configMapKeyRef:
              key: ipv4pool_ipip
              name: calico-config
        - name: FELIX_IPINIPMTU
          valueFrom:
            configMapKeyRef:
              key: veth_mtu
              name: calico-config
        - name: CALICO_IPV4POOL_CIDR
          valueFrom:
            configMapKeyRef:
              key: cluster_cidr
              name: calico-config
        - name: CALICO_DISABLE_FILE_LOGGING
          value: "true"
        - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
          value: ACCEPT
        - name: FELIX_IPV6SUPPORT
          value: "false"
        - name: FELIX_LOGSEVERITYSCREEN
          valueFrom:
            configMapKeyRef:
              key: felix_log_level
              name: calico-config
        - name: FELIX_HEALTHENABLED
          value: "true"
        image: calico/node:v3.8.2
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 6
          httpGet:
            host: localhost
            path: /liveness
            port: 9099
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: calico-node
        readinessProbe:
          exec:
            command:
            - /bin/calico-node
            - -bird-ready
            - -felix-ready
          failureThreshold: 3
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 250m
        securityContext:
          privileged: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /lib/modules
          name: lib-modules
          readOnly: true
        - mountPath: /run/xtables.lock
          name: xtables-lock
        - mountPath: /var/run/calico
          name: var-run-calico
        - mountPath: /var/lib/calico
          name: var-lib-calico
        - mountPath: /var/run/nodeagent
          name: policysync
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: calico-node-token-64zp7
          readOnly: true
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      hostNetwork: true
      initContainers:
      - command:
        - /install-cni.sh
        env:
        - name: CNI_CONF_NAME
          value: 10-calico.conflist
        - name: CNI_NETWORK_CONFIG
          valueFrom:
            configMapKeyRef:
              key: cni_network_config
              name: calico-config
        - name: KUBERNETES_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: CNI_MTU
          valueFrom:
            configMapKeyRef:
              key: veth_mtu
              name: calico-config
        - name: SLEEP
          value: "false"
        image: calico/cni:v3.8.2
        imagePullPolicy: IfNotPresent
        name: install-cni
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /host/opt/cni/bin
          name: cni-bin-dir
        - mountPath: /host/etc/cni/net.d
          name: cni-net-dir
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: calico-node-token-64zp7
          readOnly: true
      - image: calico/pod2daemon-flexvol:v3.8.2
        imagePullPolicy: IfNotPresent
        name: flexvol-driver
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /host/driver
          name: flexvol-driver-host
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: calico-node-token-64zp7
          readOnly: true
      nodeName: kind-control-plane
      nodeSelector:
        beta.kubernetes.io/os: linux
      priority: 2000001000
      priorityClassName: system-node-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: calico-node
      serviceAccountName: calico-node
      terminationGracePeriodSeconds: 0
      tolerations:
      - effect: NoSchedule
        operator: Exists
      - key: CriticalAddonsOnly
        operator: Exists
      - effect: NoExecute
        operator: Exists
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/disk-pressure
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/memory-pressure
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/pid-pressure
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/unschedulable
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/network-unavailable
        operator: Exists
      volumes:
      - hostPath:
          path: /lib/modules
          type: ""
        name: lib-modules
      - hostPath:
          path: /var/run/calico
          type: ""
        name: var-run-calico
      - hostPath:
          path: /var/lib/calico
          type: ""
        name: var-lib-calico
      - hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
        name: xtables-lock
      - hostPath:
          path: /opt/cni/bin
          type: ""
        name: cni-bin-dir
      - hostPath:
          path: /etc/cni/net.d
          type: ""
        name: cni-net-dir
      - hostPath:
          path: /var/run/nodeagent
          type: DirectoryOrCreate
        name: policysync
      - hostPath:
          path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
          type: DirectoryOrCreate
        name: flexvol-driver-host
      - name: calico-node-token-64zp7
        secret:
          defaultMode: 420
          secretName: calico-node-token-64zp7
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:42Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:56Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:56Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:35Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://f045f6a2095310897a7474fcbca401e333c6e649375dad3be22bb1fa0e36b6d2
        image: docker.io/calico/node:v3.8.2
        imageID: docker.io/calico/node@sha256:7316d804e54e884ffb0a850ee82ae28620c1b80479c045d8378d6848ee197503
        lastState: {}
        name: calico-node
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:34:45Z"
      hostIP: 172.17.0.2
      initContainerStatuses:
      - containerID: containerd://87d07149c96e476565e1de57afa4b49e7d277adc85280bfd05c9765baf7e4250
        image: docker.io/calico/cni:v3.8.2
        imageID: docker.io/calico/cni@sha256:4922215c127c18b00c8f5916997259589577c132260181a2c50a093a78564c90
        lastState: {}
        name: install-cni
        ready: true
        restartCount: 0
        state:
          terminated:
            containerID: containerd://87d07149c96e476565e1de57afa4b49e7d277adc85280bfd05c9765baf7e4250
            exitCode: 0
            finishedAt: "2020-04-27T20:34:40Z"
            reason: Completed
            startedAt: "2020-04-27T20:34:39Z"
      - containerID: containerd://331169f5b427b16200a176836f19febb80b05660ea00aa30613c38d2bf289760
        image: docker.io/calico/pod2daemon-flexvol:v3.8.2
        imageID: docker.io/calico/pod2daemon-flexvol@sha256:1dd0845153717d4bfc142a3003c210c0b53f8550f303871d7bb71fbaf52497db
        lastState: {}
        name: flexvol-driver
        ready: true
        restartCount: 0
        state:
          terminated:
            containerID: containerd://331169f5b427b16200a176836f19febb80b05660ea00aa30613c38d2bf289760
            exitCode: 0
            finishedAt: "2020-04-27T20:34:41Z"
            reason: Completed
            startedAt: "2020-04-27T20:34:41Z"
      phase: Running
      podIP: 172.17.0.2
      qosClass: Burstable
      startTime: "2020-04-27T20:34:35Z"
  - apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        cni.projectcalico.org/podIP: 100.200.82.1/32
      creationTimestamp: "2020-04-27T20:34:35Z"
      generateName: coredns-5d4dd4b4db-
      labels:
        k8s-app: kube-dns
        pod-template-hash: 5d4dd4b4db
      name: coredns-5d4dd4b4db-7w9bv
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: ReplicaSet
        name: coredns-5d4dd4b4db
        uid: 02a4e24b-1af1-4f32-ba89-fbd10e813ca9
      resourceVersion: "633"
      selfLink: /api/v1/namespaces/kube-system/pods/coredns-5d4dd4b4db-7w9bv
      uid: 829159d5-2b90-48d2-8958-0716b9e71f26
    spec:
      containers:
      - args:
        - -conf
        - /etc/coredns/Corefile
        image: k8s.gcr.io/coredns:1.3.1
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        name: coredns
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/coredns
          name: config-volume
          readOnly: true
        - mountPath: /tmp
          name: tmp
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: coredns-token-cpttt
          readOnly: true
      dnsPolicy: Default
      enableServiceLinks: true
      nodeName: kind-control-plane
      nodeSelector:
        beta.kubernetes.io/os: linux
      priority: 2000000000
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: coredns
      serviceAccountName: coredns
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 300
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 300
      volumes:
      - emptyDir: {}
        name: tmp
      - configMap:
          defaultMode: 420
          items:
          - key: Corefile
            path: Corefile
          name: coredns
        name: config-volume
      - name: coredns-token-cpttt
        secret:
          defaultMode: 420
          secretName: coredns-token-cpttt
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:47Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:50Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:50Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:47Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://e4473350db8380bf80ed0ad6129b27a34501086e5ceb37937d38d1499fa637f2
        image: k8s.gcr.io/coredns:1.3.1
        imageID: sha256:eb516548c180f8a6e0235034ccee2428027896af16a509786da13022fe95fe8c
        lastState: {}
        name: coredns
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:34:49Z"
      hostIP: 172.17.0.2
      phase: Running
      podIP: 100.200.82.1
      qosClass: Burstable
      startTime: "2020-04-27T20:34:47Z"
  - apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        cni.projectcalico.org/podIP: 100.200.82.4/32
      creationTimestamp: "2020-04-27T20:34:35Z"
      generateName: coredns-5d4dd4b4db-
      labels:
        k8s-app: kube-dns
        pod-template-hash: 5d4dd4b4db
      name: coredns-5d4dd4b4db-jp62r
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: ReplicaSet
        name: coredns-5d4dd4b4db
        uid: 02a4e24b-1af1-4f32-ba89-fbd10e813ca9
      resourceVersion: "742"
      selfLink: /api/v1/namespaces/kube-system/pods/coredns-5d4dd4b4db-jp62r
      uid: 567d95e4-e9f8-4b2a-9738-a86c4395a63e
    spec:
      containers:
      - args:
        - -conf
        - /etc/coredns/Corefile
        image: k8s.gcr.io/coredns:1.3.1
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        name: coredns
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/coredns
          name: config-volume
          readOnly: true
        - mountPath: /tmp
          name: tmp
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: coredns-token-cpttt
          readOnly: true
      dnsPolicy: Default
      enableServiceLinks: true
      nodeName: kind-control-plane
      nodeSelector:
        beta.kubernetes.io/os: linux
      priority: 2000000000
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: coredns
      serviceAccountName: coredns
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 300
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 300
      volumes:
      - emptyDir: {}
        name: tmp
      - configMap:
          defaultMode: 420
          items:
          - key: Corefile
            path: Corefile
          name: coredns
        name: config-volume
      - name: coredns-token-cpttt
        secret:
          defaultMode: 420
          secretName: coredns-token-cpttt
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:45Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:06Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:06Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:45Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://1f323b2ca51d5fe0dc1ecfbfdda0e3946ea972661f93fb60b0f71803874324f8
        image: k8s.gcr.io/coredns:1.3.1
        imageID: sha256:eb516548c180f8a6e0235034ccee2428027896af16a509786da13022fe95fe8c
        lastState: {}
        name: coredns
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:34:57Z"
      hostIP: 172.17.0.2
      phase: Running
      podIP: 100.200.82.4
      qosClass: Burstable
      startTime: "2020-04-27T20:34:45Z"
  - apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        kubernetes.io/config.hash: 932aa98260151812cbea36c55a31df20
        kubernetes.io/config.mirror: 932aa98260151812cbea36c55a31df20
        kubernetes.io/config.seen: "2020-04-27T20:34:00.471762754Z"
        kubernetes.io/config.source: file
      creationTimestamp: "2020-04-27T20:35:13Z"
      labels:
        component: etcd
        tier: control-plane
      name: etcd-kind-control-plane
      namespace: kube-system
      resourceVersion: "783"
      selfLink: /api/v1/namespaces/kube-system/pods/etcd-kind-control-plane
      uid: 3e54fbd5-2272-4bf7-b80a-5a599a69ff37
    spec:
      containers:
      - command:
        - etcd
        - --advertise-client-urls=https://172.17.0.2:2379
        - --cert-file=/etc/kubernetes/pki/etcd/server.crt
        - --client-cert-auth=true
        - --data-dir=/var/lib/etcd
        - --initial-advertise-peer-urls=https://172.17.0.2:2380
        - --initial-cluster=kind-control-plane=https://172.17.0.2:2380
        - --key-file=/etc/kubernetes/pki/etcd/server.key
        - --listen-client-urls=https://127.0.0.1:2379,https://172.17.0.2:2379
        - --listen-metrics-urls=http://0.0.0.0:2381
        - --listen-peer-urls=https://172.17.0.2:2380
        - --name=kind-control-plane
        - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
        - --peer-client-cert-auth=true
        - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
        - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
        - --snapshot-count=10000
        - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
        image: k8s.gcr.io/etcd:3.3.10
        imagePullPolicy: IfNotPresent
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -ec
            - ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt
              --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key
              get foo
          failureThreshold: 8
          initialDelaySeconds: 15
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 15
        name: etcd
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/etcd
          name: etcd-data
        - mountPath: /etc/kubernetes/pki/etcd
          name: etcd-certs
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      hostNetwork: true
      nodeName: kind-control-plane
      priority: 2000000000
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoExecute
        operator: Exists
      volumes:
      - hostPath:
          path: /etc/kubernetes/pki/etcd
          type: DirectoryOrCreate
        name: etcd-certs
      - hostPath:
          path: /var/lib/etcd
          type: DirectoryOrCreate
        name: etcd-data
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:00Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:11Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:11Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:00Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://2ff24855e3494486f2fdcc6d1bd5974b77531dc50ce469ad9214a55ef872be91
        image: k8s.gcr.io/etcd:3.3.10
        imageID: sha256:2c4adeb21b4ff8ed3309d0e42b6b4ae39872399f7b37e0856e673b13c4aba13d
        lastState: {}
        name: etcd
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:34:11Z"
      hostIP: 172.17.0.2
      phase: Running
      podIP: 172.17.0.2
      qosClass: BestEffort
      startTime: "2020-04-27T20:34:00Z"
  - apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        kubernetes.io/config.hash: 85553f871a02dde854f88a9e5d485c56
        kubernetes.io/config.mirror: 85553f871a02dde854f88a9e5d485c56
        kubernetes.io/config.seen: "2020-04-27T20:34:00.471769404Z"
        kubernetes.io/config.source: file
      creationTimestamp: "2020-04-27T20:35:20Z"
      labels:
        component: kube-apiserver
        tier: control-plane
      name: kube-apiserver-kind-control-plane
      namespace: kube-system
      resourceVersion: "848"
      selfLink: /api/v1/namespaces/kube-system/pods/kube-apiserver-kind-control-plane
      uid: 75779e1a-48e1-4fc1-8421-3f9e927e9fc9
    spec:
      containers:
      - command:
        - kube-apiserver
        - --advertise-address=172.17.0.2
        - --allow-privileged=true
        - --audit-log-format=legacy
        - --audit-log-maxage=2
        - --audit-log-maxbackup=3
        - --audit-log-maxsize=1024
        - --audit-log-path=/var/log/audit/cluster-audit.log
        - --audit-policy-file=/etc/kubernetes/policies/audit-policy.yaml
        - --authorization-mode=Node,RBAC
        - --client-ca-file=/etc/kubernetes/pki/ca.crt
        - --enable-admission-plugins=NodeRestriction
        - --enable-bootstrap-token-auth=true
        - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
        - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
        - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
        - --etcd-servers=https://127.0.0.1:2379
        - --insecure-port=0
        - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
        - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --oidc-ca-file=/etc/ssl/oidc/ingress-ca.pem
        - --oidc-client-id=kubernetes
        - --oidc-groups-claim=groups
        - --oidc-issuer-url=https://dex.127.0.0.1.nip.io
        - --oidc-username-claim=email
        - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
        - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
        - --requestheader-allowed-names=front-proxy-client
        - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
        - --requestheader-extra-headers-prefix=X-Remote-Extra-
        - --requestheader-group-headers=X-Remote-Group
        - --requestheader-username-headers=X-Remote-User
        - --secure-port=6443
        - --service-account-key-file=/etc/kubernetes/pki/sa.pub
        - --service-cluster-ip-range=10.96.0.0/12
        - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
        - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
        image: k8s.gcr.io/kube-apiserver:v1.15.7
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 8
          httpGet:
            host: 172.17.0.2
            path: /healthz
            port: 6443
            scheme: HTTPS
          initialDelaySeconds: 15
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 15
        name: kube-apiserver
        resources:
          requests:
            cpu: 250m
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/kubernetes/policies/audit-policy.yaml
          name: auditpolicy
          readOnly: true
        - mountPath: /etc/ssl/certs
          name: ca-certs
          readOnly: true
        - mountPath: /etc/ca-certificates
          name: etc-ca-certificates
          readOnly: true
        - mountPath: /etc/kubernetes/pki
          name: k8s-certs
          readOnly: true
        - mountPath: /etc/ssl/oidc/ingress-ca.pem
          name: oidc-certificates
          readOnly: true
        - mountPath: /usr/local/share/ca-certificates
          name: usr-local-share-ca-certificates
          readOnly: true
        - mountPath: /usr/share/ca-certificates
          name: usr-share-ca-certificates
          readOnly: true
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      hostNetwork: true
      nodeName: kind-control-plane
      priority: 2000000000
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoExecute
        operator: Exists
      volumes:
      - hostPath:
          path: /etc/kubernetes/policies/audit-policy.yaml
          type: File
        name: auditpolicy
      - hostPath:
          path: /etc/ssl/certs
          type: DirectoryOrCreate
        name: ca-certs
      - hostPath:
          path: /etc/ca-certificates
          type: DirectoryOrCreate
        name: etc-ca-certificates
      - hostPath:
          path: /etc/kubernetes/pki
          type: DirectoryOrCreate
        name: k8s-certs
      - hostPath:
          path: /etc/flanksource/ingress-ca/ingress-ca.crt
          type: File
        name: oidc-certificates
      - hostPath:
          path: /usr/local/share/ca-certificates
          type: DirectoryOrCreate
        name: usr-local-share-ca-certificates
      - hostPath:
          path: /usr/share/ca-certificates
          type: DirectoryOrCreate
        name: usr-share-ca-certificates
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:00Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:11Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:11Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:00Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://8936282df5bb4612aaeacbaac1eed39daa311a01a956eb293c24011a9b35b535
        image: k8s.gcr.io/kube-apiserver:v1.15.7
        imageID: sha256:c4c896a129691e9c155d5c27131eb00403ab632f503189eb2b0b15fd24e7943e
        lastState: {}
        name: kube-apiserver
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:34:11Z"
      hostIP: 172.17.0.2
      phase: Running
      podIP: 172.17.0.2
      qosClass: Burstable
      startTime: "2020-04-27T20:34:00Z"
  - apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        kubernetes.io/config.hash: 9d83afb6036487b1fcb6a62b2aa84c85
        kubernetes.io/config.mirror: 9d83afb6036487b1fcb6a62b2aa84c85
        kubernetes.io/config.seen: "2020-04-27T20:34:00.47177223Z"
        kubernetes.io/config.source: file
      creationTimestamp: "2020-04-27T20:35:26Z"
      labels:
        component: kube-controller-manager
        tier: control-plane
      name: kube-controller-manager-kind-control-plane
      namespace: kube-system
      resourceVersion: "849"
      selfLink: /api/v1/namespaces/kube-system/pods/kube-controller-manager-kind-control-plane
      uid: ed7160da-7282-45a3-bf93-7e5293022c02
    spec:
      containers:
      - command:
        - kube-controller-manager
        - --allocate-node-cidrs=true
        - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
        - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
        - --bind-address=127.0.0.1
        - --client-ca-file=/etc/kubernetes/pki/ca.crt
        - --cluster-cidr=10.244.0.0/16
        - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
        - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
        - --controllers=*,bootstrapsigner,tokencleaner
        - --enable-hostpath-provisioner=true
        - --kubeconfig=/etc/kubernetes/controller-manager.conf
        - --leader-elect=true
        - --node-cidr-mask-size=24
        - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
        - --root-ca-file=/etc/kubernetes/pki/ca.crt
        - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
        - --use-service-account-credentials=true
        image: k8s.gcr.io/kube-controller-manager:v1.15.7
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 8
          httpGet:
            host: 127.0.0.1
            path: /healthz
            port: 10252
            scheme: HTTP
          initialDelaySeconds: 15
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 15
        name: kube-controller-manager
        resources:
          requests:
            cpu: 200m
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/ssl/certs
          name: ca-certs
          readOnly: true
        - mountPath: /etc/ca-certificates
          name: etc-ca-certificates
          readOnly: true
        - mountPath: /etc/kubernetes/pki
          name: k8s-certs
          readOnly: true
        - mountPath: /etc/kubernetes/controller-manager.conf
          name: kubeconfig
          readOnly: true
        - mountPath: /usr/local/share/ca-certificates
          name: usr-local-share-ca-certificates
          readOnly: true
        - mountPath: /usr/share/ca-certificates
          name: usr-share-ca-certificates
          readOnly: true
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      hostNetwork: true
      nodeName: kind-control-plane
      priority: 2000000000
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoExecute
        operator: Exists
      volumes:
      - hostPath:
          path: /etc/ssl/certs
          type: DirectoryOrCreate
        name: ca-certs
      - hostPath:
          path: /etc/ca-certificates
          type: DirectoryOrCreate
        name: etc-ca-certificates
      - hostPath:
          path: /etc/kubernetes/pki
          type: DirectoryOrCreate
        name: k8s-certs
      - hostPath:
          path: /etc/kubernetes/controller-manager.conf
          type: FileOrCreate
        name: kubeconfig
      - hostPath:
          path: /usr/local/share/ca-certificates
          type: DirectoryOrCreate
        name: usr-local-share-ca-certificates
      - hostPath:
          path: /usr/share/ca-certificates
          type: DirectoryOrCreate
        name: usr-share-ca-certificates
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:00Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:11Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:11Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:00Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://bd0e6b9004097349ade1afaddcb74e0d3775b735eeddf3bf071442a718e900e8
        image: k8s.gcr.io/kube-controller-manager:v1.15.7
        imageID: sha256:543fbc8ae031fc6e90e0ce12268be7e9779452b6122f73a485845ae43110adb7
        lastState: {}
        name: kube-controller-manager
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:34:11Z"
      hostIP: 172.17.0.2
      phase: Running
      podIP: 172.17.0.2
      qosClass: Burstable
      startTime: "2020-04-27T20:34:00Z"
  - apiVersion: v1
    kind: Pod
    metadata:
      creationTimestamp: "2020-04-27T20:34:35Z"
      generateName: kube-proxy-
      labels:
        controller-revision-hash: 65fbbbc6cc
        k8s-app: kube-proxy
        pod-template-generation: "1"
      name: kube-proxy-cdzt5
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: DaemonSet
        name: kube-proxy
        uid: 9e0b8f56-f15d-45a2-80d0-31234933690b
      resourceVersion: "499"
      selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-cdzt5
      uid: b946081b-00f1-436c-a39e-79c69d922911
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchFields:
              - key: metadata.name
                operator: In
                values:
                - kind-control-plane
      containers:
      - command:
        - /usr/local/bin/kube-proxy
        - --config=/var/lib/kube-proxy/config.conf
        - --hostname-override=$(NODE_NAME)
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        image: k8s.gcr.io/kube-proxy:v1.15.7
        imagePullPolicy: IfNotPresent
        name: kube-proxy
        resources: {}
        securityContext:
          privileged: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/kube-proxy
          name: kube-proxy
        - mountPath: /run/xtables.lock
          name: xtables-lock
        - mountPath: /lib/modules
          name: lib-modules
          readOnly: true
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-proxy-token-86556
          readOnly: true
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      hostNetwork: true
      nodeName: kind-control-plane
      nodeSelector:
        beta.kubernetes.io/os: linux
      priority: 2000001000
      priorityClassName: system-node-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: kube-proxy
      serviceAccountName: kube-proxy
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - operator: Exists
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/disk-pressure
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/memory-pressure
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/pid-pressure
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/unschedulable
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/network-unavailable
        operator: Exists
      volumes:
      - configMap:
          defaultMode: 420
          name: kube-proxy
        name: kube-proxy
      - hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
        name: xtables-lock
      - hostPath:
          path: /lib/modules
          type: ""
        name: lib-modules
      - name: kube-proxy-token-86556
        secret:
          defaultMode: 420
          secretName: kube-proxy-token-86556
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:35Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:37Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:37Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:35Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://9e23878e6d08ad3996eda07ba565a8ea0c9f26084461a4f246936a5eac3a6c59
        image: k8s.gcr.io/kube-proxy:v1.15.7
        imageID: sha256:bf8887de1eeea8f4c065dd274b79087c51362b152e3166b6a855bb29b5d3875c
        lastState: {}
        name: kube-proxy
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:34:36Z"
      hostIP: 172.17.0.2
      phase: Running
      podIP: 172.17.0.2
      qosClass: BestEffort
      startTime: "2020-04-27T20:34:35Z"
  - apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        kubernetes.io/config.hash: 14ff2730e74c595cd255e47190f474fd
        kubernetes.io/config.mirror: 14ff2730e74c595cd255e47190f474fd
        kubernetes.io/config.seen: "2020-04-27T20:34:00.471773954Z"
        kubernetes.io/config.source: file
      creationTimestamp: "2020-04-27T20:35:27Z"
      labels:
        component: kube-scheduler
        tier: control-plane
      name: kube-scheduler-kind-control-plane
      namespace: kube-system
      resourceVersion: "850"
      selfLink: /api/v1/namespaces/kube-system/pods/kube-scheduler-kind-control-plane
      uid: 35fd3669-be7b-41ca-a33d-0b0d780865f4
    spec:
      containers:
      - command:
        - kube-scheduler
        - --bind-address=127.0.0.1
        - --kubeconfig=/etc/kubernetes/scheduler.conf
        - --leader-elect=true
        image: k8s.gcr.io/kube-scheduler:v1.15.7
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 8
          httpGet:
            host: 127.0.0.1
            path: /healthz
            port: 10251
            scheme: HTTP
          initialDelaySeconds: 15
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 15
        name: kube-scheduler
        resources:
          requests:
            cpu: 100m
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/kubernetes/scheduler.conf
          name: kubeconfig
          readOnly: true
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      hostNetwork: true
      nodeName: kind-control-plane
      priority: 2000000000
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoExecute
        operator: Exists
      volumes:
      - hostPath:
          path: /etc/kubernetes/scheduler.conf
          type: FileOrCreate
        name: kubeconfig
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:00Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:11Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:11Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:34:00Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://fcbc316e7c51d2d9e8010c6ad46cfe079835d1a9ff10b071397df3106c618bfb
        image: k8s.gcr.io/kube-scheduler:v1.15.7
        imageID: sha256:2f42d61e565fc66b0615dd66d9db62313bce8539005073e6707f21185df0f6b0
        lastState: {}
        name: kube-scheduler
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:34:10Z"
      hostIP: 172.17.0.2
      phase: Running
      podIP: 172.17.0.2
      qosClass: Burstable
      startTime: "2020-04-27T20:34:00Z"
  - apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        cni.projectcalico.org/podIP: 100.200.82.9/32
      creationTimestamp: "2020-04-27T20:35:44Z"
      generateName: kubernetes-dashboard-7d75c474bb-
      labels:
        k8s-app: kubernetes-dashboard
        pod-template-hash: 7d75c474bb
      name: kubernetes-dashboard-7d75c474bb-ggrrx
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: ReplicaSet
        name: kubernetes-dashboard-7d75c474bb
        uid: 693b795b-0fa7-4c83-be3d-e14db10528a6
      resourceVersion: "1171"
      selfLink: /api/v1/namespaces/kube-system/pods/kubernetes-dashboard-7d75c474bb-ggrrx
      uid: eb19c973-696e-456f-b2e9-5e1c6de805f1
    spec:
      containers:
      - args:
        - --auto-generate-certificates
        image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 30
        name: kubernetes-dashboard
        ports:
        - containerPort: 8443
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /certs
          name: kubernetes-dashboard-certs
        - mountPath: /tmp
          name: tmp-volume
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kubernetes-dashboard-token-dqn9t
          readOnly: true
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      nodeName: kind-control-plane
      priority: 0
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: kubernetes-dashboard
      serviceAccountName: kubernetes-dashboard
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 300
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 300
      volumes:
      - name: kubernetes-dashboard-certs
        secret:
          defaultMode: 420
          secretName: kubernetes-dashboard-certs
      - emptyDir: {}
        name: tmp-volume
      - name: kubernetes-dashboard-token-dqn9t
        secret:
          defaultMode: 420
          secretName: kubernetes-dashboard-token-dqn9t
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:44Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:49Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:49Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:44Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://2df8a5e5678a99380205803d0e4809f12de186ac49ac1018e16de98e3219c31e
        image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
        imageID: k8s.gcr.io/kubernetes-dashboard-amd64@sha256:0ae6b69432e78069c5ce2bcde0fe409c5c4d6f0f4d9cd50a17974fea38898747
        lastState: {}
        name: kubernetes-dashboard
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:35:48Z"
      hostIP: 172.17.0.2
      phase: Running
      podIP: 100.200.82.9
      qosClass: BestEffort
      startTime: "2020-04-27T20:35:44Z"
  - apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        cni.projectcalico.org/podIP: 100.200.82.10/32
      creationTimestamp: "2020-04-27T20:35:47Z"
      generateName: namespace-configuration-operator-6f748b789c-
      labels:
        name: namespace-configuration-operator
        pod-template-hash: 6f748b789c
      name: namespace-configuration-operator-6f748b789c-p2v6j
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: ReplicaSet
        name: namespace-configuration-operator-6f748b789c
        uid: c2b7e9e4-fd04-4c2b-b1ce-24d16a201646
      resourceVersion: "1268"
      selfLink: /api/v1/namespaces/kube-system/pods/namespace-configuration-operator-6f748b789c-p2v6j
      uid: 1e147dfe-dce3-406e-89eb-79fec6c40830
    spec:
      containers:
      - command:
        - namespace-configuration-operator
        env:
        - name: WATCH_NAMESPACE
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: OPERATOR_NAME
          value: namespace-configuration-operator
        image: quay.io/redhat-cop/namespace-configuration-operator:v0.1.0
        imagePullPolicy: IfNotPresent
        name: namespace-configuration-operator
        resources:
          limits:
            cpu: 100m
            memory: 256Mi
          requests:
            cpu: 10m
            memory: 20Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: namespace-configuration-operator-token-xxbjf
          readOnly: true
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      nodeName: kind-control-plane
      priority: 0
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: namespace-configuration-operator
      serviceAccountName: namespace-configuration-operator
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 300
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 300
      volumes:
      - name: namespace-configuration-operator-token-xxbjf
        secret:
          defaultMode: 420
          secretName: namespace-configuration-operator-token-xxbjf
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:47Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:53Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:53Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:47Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://78d48c5126c4aebad4e69ab0b06f6776ce3ed3da0ab2cdef87f49d1240b4aefc
        image: quay.io/redhat-cop/namespace-configuration-operator:v0.1.0
        imageID: sha256:ee213f7ff871e7501d1f71473f29b5a88626551a364fd43f2323347020baf21d
        lastState: {}
        name: namespace-configuration-operator
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:35:52Z"
      hostIP: 172.17.0.2
      phase: Running
      podIP: 100.200.82.10
      qosClass: Burstable
      startTime: "2020-04-27T20:35:47Z"
  - apiVersion: v1
    kind: Pod
    metadata:
      creationTimestamp: "2020-04-27T20:35:28Z"
      generateName: node-local-dns-
      labels:
        controller-revision-hash: 5b88bcd6f
        k8s-app: node-local-dns
        pod-template-generation: "1"
      name: node-local-dns-ttl7q
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: DaemonSet
        name: node-local-dns
        uid: e15f3924-aa24-4407-890f-f66c84e4e7a2
      resourceVersion: "853"
      selfLink: /api/v1/namespaces/kube-system/pods/node-local-dns-ttl7q
      uid: f32262e4-7b07-46cb-b5a1-425af7faaa3c
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchFields:
              - key: metadata.name
                operator: In
                values:
                - kind-control-plane
      containers:
      - args:
        - -localip
        - 169.254.20.10,10.96.0.10
        - -conf
        - /etc/Corefile
        - -upstreamsvc
        - kube-dns-upstream
        image: k8s.gcr.io/k8s-dns-node-cache:1.15.7
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            host: 169.254.20.10
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        name: node-cache
        ports:
        - containerPort: 53
          hostPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          hostPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9253
          hostPort: 9253
          name: metrics
          protocol: TCP
        resources:
          requests:
            cpu: 25m
            memory: 5Mi
        securityContext:
          privileged: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /run/xtables.lock
          name: xtables-lock
        - mountPath: /etc/coredns
          name: config-volume
        - mountPath: /etc/kube-dns
          name: kube-dns-config
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: node-local-dns-token-bxf2k
          readOnly: true
      dnsPolicy: Default
      enableServiceLinks: true
      hostNetwork: true
      nodeName: kind-control-plane
      priority: 2000001000
      priorityClassName: system-node-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: node-local-dns
      serviceAccountName: node-local-dns
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/disk-pressure
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/memory-pressure
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/pid-pressure
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/unschedulable
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/network-unavailable
        operator: Exists
      volumes:
      - hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
        name: xtables-lock
      - configMap:
          defaultMode: 420
          name: kube-dns
          optional: true
        name: kube-dns-config
      - configMap:
          defaultMode: 420
          items:
          - key: Corefile
            path: Corefile.base
          name: node-local-dns
        name: config-volume
      - name: node-local-dns-token-bxf2k
        secret:
          defaultMode: 420
          secretName: node-local-dns-token-bxf2k
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:28Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:30Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:30Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:28Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://d7533bbfb5c63f8e1d839a6145f276456bc463bd48f35f1e1ae136619907aab1
        image: k8s.gcr.io/k8s-dns-node-cache:1.15.7
        imageID: k8s.gcr.io/k8s-dns-node-cache@sha256:25d0b564f8203c7250c055033c5b3506bc3a11ff2b461c8b6d1b256e7dd27f8a
        lastState: {}
        name: node-cache
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:35:30Z"
      hostIP: 172.17.0.2
      phase: Running
      podIP: 172.17.0.2
      qosClass: Burstable
      startTime: "2020-04-27T20:35:28Z"
  - apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        cni.projectcalico.org/podIP: 100.200.82.11/32
      creationTimestamp: "2020-04-27T20:35:48Z"
      generateName: platform-operator-995b7fb78-
      labels:
        name: platform-operator
        pod-template-hash: 995b7fb78
      name: platform-operator-995b7fb78-9m4pj
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: ReplicaSet
        name: platform-operator-995b7fb78
        uid: 711594c6-dc79-4eaf-a72f-03fb8e1244d4
      resourceVersion: "1333"
      selfLink: /api/v1/namespaces/kube-system/pods/platform-operator-995b7fb78-9m4pj
      uid: 319a5d64-4bbe-4a08-af39-51301e46da3c
    spec:
      containers:
      - image: docker.io/moshloop/platform-operator:0.1
        imagePullPolicy: IfNotPresent
        name: platform-operator
        resources:
          limits:
            cpu: 100m
            memory: 256Mi
          requests:
            cpu: 10m
            memory: 20Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: platform-operator-token-fhmf8
          readOnly: true
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      nodeName: kind-control-plane
      priority: 0
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: platform-operator
      serviceAccountName: platform-operator
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 300
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 300
      volumes:
      - name: platform-operator-token-fhmf8
        secret:
          defaultMode: 420
          secretName: platform-operator-token-fhmf8
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:48Z"
        status: "True"
        type: Initialized
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:55Z"
        status: "True"
        type: Ready
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:55Z"
        status: "True"
        type: ContainersReady
      - lastProbeTime: null
        lastTransitionTime: "2020-04-27T20:35:48Z"
        status: "True"
        type: PodScheduled
      containerStatuses:
      - containerID: containerd://6a822e9dd7783aa470d624b8b87862946220025fa1dd04527fbc04e6b5c978d9
        image: docker.io/moshloop/platform-operator:0.1
        imageID: docker.io/moshloop/platform-operator@sha256:dffe74877dfd1c9fe1ffa90e7692250663aa0753fd1e90f83f49437d98d1387c
        lastState: {}
        name: platform-operator
        ready: true
        restartCount: 0
        state:
          running:
            startedAt: "2020-04-27T20:35:55Z"
      hostIP: 172.17.0.2
      phase: Running
      podIP: 100.200.82.11
      qosClass: Burstable
      startTime: "2020-04-27T20:35:48Z"
  metadata: {}
- items:
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:19Z"
      name: attachdetach-controller
      namespace: kube-system
      resourceVersion: "226"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/attachdetach-controller
      uid: e0aa2101-c5e3-4014-9776-47e244d4dd56
    secrets:
    - name: attachdetach-controller-token-xwvnn
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:20Z"
      name: bootstrap-signer
      namespace: kube-system
      resourceVersion: "246"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/bootstrap-signer
      uid: 77b75016-3f06-4bfb-97f8-1db45b090d77
    secrets:
    - name: bootstrap-signer-token-pvnfk
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:31Z"
      name: calico-kube-controllers
      namespace: kube-system
      resourceVersion: "2022"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/calico-kube-controllers
      uid: f205cb9f-b63e-4ef1-a300-66bda22617ab
    secrets:
    - name: calico-kube-controllers-token-wfx7g
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:30Z"
      name: calico-node
      namespace: kube-system
      resourceVersion: "2012"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/calico-node
      uid: bc123614-1d64-485d-9932-5513476954ef
    secrets:
    - name: calico-node-token-54j4q
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:20Z"
      name: certificate-controller
      namespace: kube-system
      resourceVersion: "241"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/certificate-controller
      uid: 07ff0d4f-05f5-4f5a-865f-d4b2792b49d9
    secrets:
    - name: certificate-controller-token-lxnxf
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:24Z"
      name: clusterrole-aggregation-controller
      namespace: kube-system
      resourceVersion: "328"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/clusterrole-aggregation-controller
      uid: a821fc0f-ca49-4332-9919-4f20b7713b83
    secrets:
    - name: clusterrole-aggregation-controller-token-9b997
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:17Z"
      name: coredns
      namespace: kube-system
      resourceVersion: "198"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/coredns
      uid: 36020fd9-1dc4-4bf6-8083-5d4960aae8ea
    secrets:
    - name: coredns-token-cpttt
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:23Z"
      name: cronjob-controller
      namespace: kube-system
      resourceVersion: "303"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/cronjob-controller
      uid: b0538b6b-fcb8-4f77-bbd5-f89263a6896a
    secrets:
    - name: cronjob-controller-token-7qzxq
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:23Z"
      name: daemon-set-controller
      namespace: kube-system
      resourceVersion: "293"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/daemon-set-controller
      uid: 50bb9d3f-2698-49f8-b32e-cd22579a96d8
    secrets:
    - name: daemon-set-controller-token-z9954
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:35Z"
      name: default
      namespace: kube-system
      resourceVersion: "442"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/default
      uid: 40a9e50d-094c-4242-aa77-347db508091d
    secrets:
    - name: default-token-4s55r
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:20Z"
      name: deployment-controller
      namespace: kube-system
      resourceVersion: "238"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/deployment-controller
      uid: cfcde99f-5d03-4448-84c2-1f61b8040738
    secrets:
    - name: deployment-controller-token-gdpkx
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:18Z"
      name: disruption-controller
      namespace: kube-system
      resourceVersion: "221"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/disruption-controller
      uid: 2d332787-0731-44e8-aad7-ccbe25ae3308
    secrets:
    - name: disruption-controller-token-h559h
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:18Z"
      name: endpoint-controller
      namespace: kube-system
      resourceVersion: "214"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/endpoint-controller
      uid: b3dbc34c-37a8-4890-a416-8bf1f3be7120
    secrets:
    - name: endpoint-controller-token-g95bz
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:24Z"
      name: expand-controller
      namespace: kube-system
      resourceVersion: "311"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/expand-controller
      uid: ac2995da-b614-4e90-9f3d-cd143dd12606
    secrets:
    - name: expand-controller-token-5wt75
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:19Z"
      name: generic-garbage-collector
      namespace: kube-system
      resourceVersion: "230"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/generic-garbage-collector
      uid: 4e72a14e-91f0-4d3c-b66e-7265b1e0d472
    secrets:
    - name: generic-garbage-collector-token-7ntmf
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:21Z"
      name: horizontal-pod-autoscaler
      namespace: kube-system
      resourceVersion: "273"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/horizontal-pod-autoscaler
      uid: 3542a821-8de8-460a-a060-d53bfc1a6802
    secrets:
    - name: horizontal-pod-autoscaler-token-wmp5l
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:18Z"
      name: job-controller
      namespace: kube-system
      resourceVersion: "205"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/job-controller
      uid: 701bb0d8-5602-4988-91be-36e1683185d1
    secrets:
    - name: job-controller-token-d22k8
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:17Z"
      name: kube-proxy
      namespace: kube-system
      resourceVersion: "199"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/kube-proxy
      uid: 4bd2f410-6776-4c39-960c-51c4ec1f2c44
    secrets:
    - name: kube-proxy-token-86556
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:35:43Z"
      labels:
        k8s-app: kubernetes-dashboard
      name: kubernetes-dashboard
      namespace: kube-system
      resourceVersion: "2205"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/kubernetes-dashboard
      uid: 88671a61-53b1-4cf8-9a33-e9f56d13168b
    secrets:
    - name: kubernetes-dashboard-token-krq8r
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:35:46Z"
      name: namespace-configuration-operator
      namespace: kube-system
      resourceVersion: "2219"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/namespace-configuration-operator
      uid: c3794c93-d617-43e0-aa69-553ffc51eabe
    secrets:
    - name: namespace-configuration-operator-token-w8jdt
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:21Z"
      name: namespace-controller
      namespace: kube-system
      resourceVersion: "263"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/namespace-controller
      uid: 8886a322-fa89-454c-8a06-e3c865938191
    secrets:
    - name: namespace-controller-token-2qlqh
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:18Z"
      name: node-controller
      namespace: kube-system
      resourceVersion: "211"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/node-controller
      uid: 048e62eb-8f2b-4a94-ae90-c18d64956015
    secrets:
    - name: node-controller-token-ffnp8
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:35:28Z"
      labels:
        addonmanager.kubernetes.io/mode: Reconcile
        kubernetes.io/cluster-service: "true"
      name: node-local-dns
      namespace: kube-system
      resourceVersion: "2112"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/node-local-dns
      uid: f37ac732-1434-4b47-8f49-0afc5908f178
    secrets:
    - name: node-local-dns-token-w6btm
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:20Z"
      name: persistent-volume-binder
      namespace: kube-system
      resourceVersion: "255"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/persistent-volume-binder
      uid: 8bfd3d4c-0a46-41d9-bcfc-f65b778346b0
    secrets:
    - name: persistent-volume-binder-token-2d57l
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:35:47Z"
      name: platform-operator
      namespace: kube-system
      resourceVersion: "2228"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/platform-operator
      uid: 49868e28-a7cf-40e6-9349-c666eba8beea
    secrets:
    - name: platform-operator-token-22mwh
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:18Z"
      name: pod-garbage-collector
      namespace: kube-system
      resourceVersion: "200"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/pod-garbage-collector
      uid: 775ca60a-d925-4cdf-9bda-28c3c30685d6
    secrets:
    - name: pod-garbage-collector-token-nqxk2
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:22Z"
      name: pv-protection-controller
      namespace: kube-system
      resourceVersion: "285"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/pv-protection-controller
      uid: 50872ae3-aab2-4e55-be83-b39bfe8ec841
    secrets:
    - name: pv-protection-controller-token-kkkvj
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:21Z"
      name: pvc-protection-controller
      namespace: kube-system
      resourceVersion: "259"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/pvc-protection-controller
      uid: f64ab52a-ec1f-4345-b26c-0a228b46c7e0
    secrets:
    - name: pvc-protection-controller-token-mxdvn
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:18Z"
      name: replicaset-controller
      namespace: kube-system
      resourceVersion: "208"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/replicaset-controller
      uid: f996b920-00d6-4b04-a6f7-0674d9157890
    secrets:
    - name: replicaset-controller-token-bcsrr
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:23Z"
      name: replication-controller
      namespace: kube-system
      resourceVersion: "289"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/replication-controller
      uid: fd6976ea-88a3-4b4a-b2e9-77bc1750f0a3
    secrets:
    - name: replication-controller-token-qt4cs
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:18Z"
      name: resourcequota-controller
      namespace: kube-system
      resourceVersion: "217"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/resourcequota-controller
      uid: 368943d3-7ce7-45fd-bda9-d9e8f0682940
    secrets:
    - name: resourcequota-controller-token-tlz8n
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:21Z"
      name: service-account-controller
      namespace: kube-system
      resourceVersion: "268"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/service-account-controller
      uid: c46b8652-e970-4058-9fd7-cbb411e4f448
    secrets:
    - name: service-account-controller-token-pxxdv
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:24Z"
      name: service-controller
      namespace: kube-system
      resourceVersion: "307"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/service-controller
      uid: 1100b496-0a14-4fee-9aaf-92a3f42b062e
    secrets:
    - name: service-controller-token-w2qts
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:23Z"
      name: statefulset-controller
      namespace: kube-system
      resourceVersion: "297"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/statefulset-controller
      uid: efc23ac0-0bba-404a-a7a8-3f67717ec086
    secrets:
    - name: statefulset-controller-token-w6tpx
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:20Z"
      name: token-cleaner
      namespace: kube-system
      resourceVersion: "251"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/token-cleaner
      uid: c5070ddf-3578-4c4b-800e-2ade4b302ddb
    secrets:
    - name: token-cleaner-token-tk2h7
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      creationTimestamp: "2020-04-27T20:34:22Z"
      name: ttl-controller
      namespace: kube-system
      resourceVersion: "280"
      selfLink: /api/v1/namespaces/kube-system/serviceaccounts/ttl-controller
      uid: 7135cd18-54b9-42e3-9184-ab06401f4e6d
    secrets:
    - name: ttl-controller-token-tcmtf
  metadata: {}
- items:
  - apiVersion: v1
    kind: Service
    metadata:
      creationTimestamp: "2020-04-27T20:40:30Z"
      labels:
        component: etcd
        tier: control-plane
      name: etcd-discovery
      namespace: kube-system
      resourceVersion: "3937"
      selfLink: /api/v1/namespaces/kube-system/services/etcd-discovery
      uid: cf7d7281-fdb3-438c-b9ca-b93f51dab760
    spec:
      clusterIP: 10.100.46.28
      ports:
      - name: http-metrics
        port: 2381
        protocol: TCP
        targetPort: 2381
      selector:
        component: etcd
        tier: control-plane
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}
  - apiVersion: v1
    kind: Service
    metadata:
      creationTimestamp: "2020-04-27T20:40:31Z"
      labels:
        k8s-app: kube-controller-manager
      name: kube-controller-manager-prometheus-discovery
      namespace: kube-system
      resourceVersion: "3947"
      selfLink: /api/v1/namespaces/kube-system/services/kube-controller-manager-prometheus-discovery
      uid: 234e48fa-3179-427d-9ff3-efd3cd33fc47
    spec:
      clusterIP: 10.104.246.22
      ports:
      - name: http-metrics
        port: 10252
        protocol: TCP
        targetPort: 10252
      selector:
        component: kube-controller-manager
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        prometheus.io/port: "9153"
        prometheus.io/scrape: "true"
      creationTimestamp: "2020-04-27T20:34:17Z"
      labels:
        k8s-app: kube-dns
        kubernetes.io/cluster-service: "true"
        kubernetes.io/name: KubeDNS
      name: kube-dns
      namespace: kube-system
      resourceVersion: "173"
      selfLink: /api/v1/namespaces/kube-system/services/kube-dns
      uid: 43d35cd0-f40a-4a42-ab53-247e701ab25f
    spec:
      clusterIP: 10.96.0.10
      ports:
      - name: dns
        port: 53
        protocol: UDP
        targetPort: 53
      - name: dns-tcp
        port: 53
        protocol: TCP
        targetPort: 53
      - name: metrics
        port: 9153
        protocol: TCP
        targetPort: 9153
      selector:
        k8s-app: kube-dns
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}
  - apiVersion: v1
    kind: Service
    metadata:
      creationTimestamp: "2020-04-27T20:35:28Z"
      labels:
        addonmanager.kubernetes.io/mode: Reconcile
        k8s-app: kube-dns
        kubernetes.io/cluster-service: "true"
        kubernetes.io/name: KubeDNSUpstream
      name: kube-dns-upstream
      namespace: kube-system
      resourceVersion: "813"
      selfLink: /api/v1/namespaces/kube-system/services/kube-dns-upstream
      uid: 48e788bc-d789-4c2c-9409-46c2d024c641
    spec:
      clusterIP: 10.97.46.160
      ports:
      - name: dns
        port: 53
        protocol: UDP
        targetPort: 53
      - name: dns-tcp
        port: 53
        protocol: TCP
        targetPort: 53
      selector:
        k8s-app: kube-dns
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}
  - apiVersion: v1
    kind: Service
    metadata:
      creationTimestamp: "2020-04-27T20:40:30Z"
      labels:
        k8s-app: kube-scheduler
      name: kube-scheduler-prometheus-discovery
      namespace: kube-system
      resourceVersion: "3943"
      selfLink: /api/v1/namespaces/kube-system/services/kube-scheduler-prometheus-discovery
      uid: fd571d1d-8fff-4fbb-b679-26c343121095
    spec:
      clusterIP: 10.99.217.17
      ports:
      - name: http-metrics
        port: 10251
        protocol: TCP
        targetPort: 10251
      selector:
        component: kube-scheduler
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}
  - apiVersion: v1
    kind: Service
    metadata:
      creationTimestamp: "2020-04-27T20:40:25Z"
      labels:
        k8s-app: kubelet
      name: kubelet
      namespace: kube-system
      resourceVersion: "3765"
      selfLink: /api/v1/namespaces/kube-system/services/kubelet
      uid: c79b5645-ebd8-4a1b-b229-15039f712168
    spec:
      clusterIP: None
      ports:
      - name: https-metrics
        port: 10250
        protocol: TCP
        targetPort: 10250
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}
  - apiVersion: v1
    kind: Service
    metadata:
      creationTimestamp: "2020-04-27T20:35:45Z"
      labels:
        k8s-app: kubernetes-dashboard
      name: kubernetes-dashboard
      namespace: kube-system
      resourceVersion: "1059"
      selfLink: /api/v1/namespaces/kube-system/services/kubernetes-dashboard
      uid: bce41a84-46d4-4742-b36d-03b2417c6a7b
    spec:
      clusterIP: 10.100.249.13
      ports:
      - port: 443
        protocol: TCP
        targetPort: 8443
      selector:
        k8s-app: kubernetes-dashboard
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}
  - apiVersion: v1
    kind: Service
    metadata:
      creationTimestamp: "2020-04-27T20:35:54Z"
      labels:
        name: namespace-configuration-operator
      name: namespace-configuration-operator-metrics
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: Deployment
        name: namespace-configuration-operator
        uid: 90a9a6de-da61-46a1-9d08-174f551e173e
      resourceVersion: "1285"
      selfLink: /api/v1/namespaces/kube-system/services/namespace-configuration-operator-metrics
      uid: c5eacbc3-bd75-4eb7-a91d-d9f4e3c888a0
    spec:
      clusterIP: 10.109.118.78
      ports:
      - name: http-metrics
        port: 8383
        protocol: TCP
        targetPort: 8383
      - name: cr-metrics
        port: 8686
        protocol: TCP
        targetPort: 8686
      selector:
        name: namespace-configuration-operator
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}
  metadata: {}
- items:
  - apiVersion: apps/v1
    data:
      spec:
        template:
          $patch: replace
          metadata:
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ""
            creationTimestamp: null
            labels:
              k8s-app: calico-node
          spec:
            containers:
            - env:
              - name: FELIX_IGNORELOOSERPF
                value: "true"
              - name: DATASTORE_TYPE
                value: kubernetes
              - name: WAIT_FOR_DATASTORE
                value: "true"
              - name: NODENAME
                valueFrom:
                  fieldRef:
                    apiVersion: v1
                    fieldPath: spec.nodeName
              - name: CALICO_NETWORKING_BACKEND
                valueFrom:
                  configMapKeyRef:
                    key: calico_backend
                    name: calico-config
              - name: CLUSTER_TYPE
                valueFrom:
                  configMapKeyRef:
                    key: cluster_type
                    name: calico-config
              - name: IP
                value: autodetect
              - name: CALICO_IPV4POOL_IPIP
                valueFrom:
                  configMapKeyRef:
                    key: ipv4pool_ipip
                    name: calico-config
              - name: FELIX_IPINIPMTU
                valueFrom:
                  configMapKeyRef:
                    key: veth_mtu
                    name: calico-config
              - name: CALICO_IPV4POOL_CIDR
                valueFrom:
                  configMapKeyRef:
                    key: cluster_cidr
                    name: calico-config
              - name: CALICO_DISABLE_FILE_LOGGING
                value: "true"
              - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
                value: ACCEPT
              - name: FELIX_IPV6SUPPORT
                value: "false"
              - name: FELIX_LOGSEVERITYSCREEN
                valueFrom:
                  configMapKeyRef:
                    key: felix_log_level
                    name: calico-config
              - name: FELIX_HEALTHENABLED
                value: "true"
              image: calico/node:v3.8.2
              imagePullPolicy: IfNotPresent
              livenessProbe:
                failureThreshold: 6
                httpGet:
                  host: localhost
                  path: /liveness
                  port: 9099
                  scheme: HTTP
                initialDelaySeconds: 10
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 1
              name: calico-node
              readinessProbe:
                exec:
                  command:
                  - /bin/calico-node
                  - -bird-ready
                  - -felix-ready
                failureThreshold: 3
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 1
              resources:
                requests:
                  cpu: 250m
              securityContext:
                privileged: true
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /lib/modules
                name: lib-modules
                readOnly: true
              - mountPath: /run/xtables.lock
                name: xtables-lock
              - mountPath: /var/run/calico
                name: var-run-calico
              - mountPath: /var/lib/calico
                name: var-lib-calico
              - mountPath: /var/run/nodeagent
                name: policysync
            dnsPolicy: ClusterFirst
            hostNetwork: true
            initContainers:
            - command:
              - /install-cni.sh
              env:
              - name: CNI_CONF_NAME
                value: 10-calico.conflist
              - name: CNI_NETWORK_CONFIG
                valueFrom:
                  configMapKeyRef:
                    key: cni_network_config
                    name: calico-config
              - name: KUBERNETES_NODE_NAME
                valueFrom:
                  fieldRef:
                    apiVersion: v1
                    fieldPath: spec.nodeName
              - name: CNI_MTU
                valueFrom:
                  configMapKeyRef:
                    key: veth_mtu
                    name: calico-config
              - name: SLEEP
                value: "false"
              image: calico/cni:v3.8.2
              imagePullPolicy: IfNotPresent
              name: install-cni
              resources: {}
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /host/opt/cni/bin
                name: cni-bin-dir
              - mountPath: /host/etc/cni/net.d
                name: cni-net-dir
            - image: calico/pod2daemon-flexvol:v3.8.2
              imagePullPolicy: IfNotPresent
              name: flexvol-driver
              resources: {}
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /host/driver
                name: flexvol-driver-host
            nodeSelector:
              beta.kubernetes.io/os: linux
            priorityClassName: system-node-critical
            restartPolicy: Always
            schedulerName: default-scheduler
            securityContext: {}
            serviceAccount: calico-node
            serviceAccountName: calico-node
            terminationGracePeriodSeconds: 0
            tolerations:
            - effect: NoSchedule
              operator: Exists
            - key: CriticalAddonsOnly
              operator: Exists
            - effect: NoExecute
              operator: Exists
            volumes:
            - hostPath:
                path: /lib/modules
                type: ""
              name: lib-modules
            - hostPath:
                path: /var/run/calico
                type: ""
              name: var-run-calico
            - hostPath:
                path: /var/lib/calico
                type: ""
              name: var-lib-calico
            - hostPath:
                path: /run/xtables.lock
                type: FileOrCreate
              name: xtables-lock
            - hostPath:
                path: /opt/cni/bin
                type: ""
              name: cni-bin-dir
            - hostPath:
                path: /etc/cni/net.d
                type: ""
              name: cni-net-dir
            - hostPath:
                path: /var/run/nodeagent
                type: DirectoryOrCreate
              name: policysync
            - hostPath:
                path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
                type: DirectoryOrCreate
              name: flexvol-driver-host
    kind: ControllerRevision
    metadata:
      annotations:
        deprecated.daemonset.template.generation: "1"
      creationTimestamp: "2020-04-27T20:34:35Z"
      labels:
        controller-revision-hash: fc88f6b8b
        k8s-app: calico-node
      name: calico-node-fc88f6b8b
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: DaemonSet
        name: calico-node
        uid: 7bf1cc4a-c741-466f-9ab4-ed1d4fb6d471
      resourceVersion: "414"
      selfLink: /apis/apps/v1/namespaces/kube-system/controllerrevisions/calico-node-fc88f6b8b
      uid: 813ed619-50b3-4714-81dd-37595a81e5ac
    revision: 1
  - apiVersion: apps/v1
    data:
      spec:
        template:
          $patch: replace
          metadata:
            creationTimestamp: null
            labels:
              k8s-app: kube-proxy
          spec:
            containers:
            - command:
              - /usr/local/bin/kube-proxy
              - --config=/var/lib/kube-proxy/config.conf
              - --hostname-override=$(NODE_NAME)
              env:
              - name: NODE_NAME
                valueFrom:
                  fieldRef:
                    apiVersion: v1
                    fieldPath: spec.nodeName
              image: k8s.gcr.io/kube-proxy:v1.15.7
              imagePullPolicy: IfNotPresent
              name: kube-proxy
              resources: {}
              securityContext:
                privileged: true
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /var/lib/kube-proxy
                name: kube-proxy
              - mountPath: /run/xtables.lock
                name: xtables-lock
              - mountPath: /lib/modules
                name: lib-modules
                readOnly: true
            dnsPolicy: ClusterFirst
            hostNetwork: true
            nodeSelector:
              beta.kubernetes.io/os: linux
            priorityClassName: system-node-critical
            restartPolicy: Always
            schedulerName: default-scheduler
            securityContext: {}
            serviceAccount: kube-proxy
            serviceAccountName: kube-proxy
            terminationGracePeriodSeconds: 30
            tolerations:
            - key: CriticalAddonsOnly
              operator: Exists
            - operator: Exists
            volumes:
            - configMap:
                defaultMode: 420
                name: kube-proxy
              name: kube-proxy
            - hostPath:
                path: /run/xtables.lock
                type: FileOrCreate
              name: xtables-lock
            - hostPath:
                path: /lib/modules
                type: ""
              name: lib-modules
    kind: ControllerRevision
    metadata:
      annotations:
        deprecated.daemonset.template.generation: "1"
      creationTimestamp: "2020-04-27T20:34:35Z"
      labels:
        controller-revision-hash: 65fbbbc6cc
        k8s-app: kube-proxy
      name: kube-proxy-65fbbbc6cc
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: DaemonSet
        name: kube-proxy
        uid: 9e0b8f56-f15d-45a2-80d0-31234933690b
      resourceVersion: "413"
      selfLink: /apis/apps/v1/namespaces/kube-system/controllerrevisions/kube-proxy-65fbbbc6cc
      uid: 61ba118b-2649-4507-94c8-ef2bb5478a05
    revision: 1
  - apiVersion: apps/v1
    data:
      spec:
        template:
          $patch: replace
          metadata:
            creationTimestamp: null
            labels:
              k8s-app: node-local-dns
          spec:
            containers:
            - args:
              - -localip
              - 169.254.20.10,10.96.0.10
              - -conf
              - /etc/Corefile
              - -upstreamsvc
              - kube-dns-upstream
              image: k8s.gcr.io/k8s-dns-node-cache:1.15.7
              imagePullPolicy: IfNotPresent
              livenessProbe:
                failureThreshold: 3
                httpGet:
                  host: 169.254.20.10
                  path: /health
                  port: 8080
                  scheme: HTTP
                initialDelaySeconds: 60
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 5
              name: node-cache
              ports:
              - containerPort: 53
                hostPort: 53
                name: dns
                protocol: UDP
              - containerPort: 53
                hostPort: 53
                name: dns-tcp
                protocol: TCP
              - containerPort: 9253
                hostPort: 9253
                name: metrics
                protocol: TCP
              resources:
                requests:
                  cpu: 25m
                  memory: 5Mi
              securityContext:
                privileged: true
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /run/xtables.lock
                name: xtables-lock
              - mountPath: /etc/coredns
                name: config-volume
              - mountPath: /etc/kube-dns
                name: kube-dns-config
            dnsPolicy: Default
            hostNetwork: true
            priorityClassName: system-node-critical
            restartPolicy: Always
            schedulerName: default-scheduler
            securityContext: {}
            serviceAccount: node-local-dns
            serviceAccountName: node-local-dns
            terminationGracePeriodSeconds: 30
            tolerations:
            - key: CriticalAddonsOnly
              operator: Exists
            volumes:
            - hostPath:
                path: /run/xtables.lock
                type: FileOrCreate
              name: xtables-lock
            - configMap:
                defaultMode: 420
                name: kube-dns
                optional: true
              name: kube-dns-config
            - configMap:
                defaultMode: 420
                items:
                - key: Corefile
                  path: Corefile.base
                name: node-local-dns
              name: config-volume
    kind: ControllerRevision
    metadata:
      annotations:
        deprecated.daemonset.template.generation: "1"
      creationTimestamp: "2020-04-27T20:35:28Z"
      labels:
        controller-revision-hash: 5b88bcd6f
        k8s-app: node-local-dns
      name: node-local-dns-5b88bcd6f
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: DaemonSet
        name: node-local-dns
        uid: e15f3924-aa24-4407-890f-f66c84e4e7a2
      resourceVersion: "818"
      selfLink: /apis/apps/v1/namespaces/kube-system/controllerrevisions/node-local-dns-5b88bcd6f
      uid: f7caae5b-434e-45b2-914d-b0b2a4b4341d
    revision: 1
  metadata: {}
- items:
  - apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      annotations:
        deprecated.daemonset.template.generation: "1"
      creationTimestamp: "2020-04-27T20:34:30Z"
      generation: 1
      labels:
        k8s-app: calico-node
      name: calico-node
      namespace: kube-system
      resourceVersion: "677"
      selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/calico-node
      uid: 7bf1cc4a-c741-466f-9ab4-ed1d4fb6d471
    spec:
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          k8s-app: calico-node
      template:
        metadata:
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ""
          creationTimestamp: null
          labels:
            k8s-app: calico-node
        spec:
          containers:
          - env:
            - name: FELIX_IGNORELOOSERPF
              value: "true"
            - name: DATASTORE_TYPE
              value: kubernetes
            - name: WAIT_FOR_DATASTORE
              value: "true"
            - name: NODENAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CALICO_NETWORKING_BACKEND
              valueFrom:
                configMapKeyRef:
                  key: calico_backend
                  name: calico-config
            - name: CLUSTER_TYPE
              valueFrom:
                configMapKeyRef:
                  key: cluster_type
                  name: calico-config
            - name: IP
              value: autodetect
            - name: CALICO_IPV4POOL_IPIP
              valueFrom:
                configMapKeyRef:
                  key: ipv4pool_ipip
                  name: calico-config
            - name: FELIX_IPINIPMTU
              valueFrom:
                configMapKeyRef:
                  key: veth_mtu
                  name: calico-config
            - name: CALICO_IPV4POOL_CIDR
              valueFrom:
                configMapKeyRef:
                  key: cluster_cidr
                  name: calico-config
            - name: CALICO_DISABLE_FILE_LOGGING
              value: "true"
            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
              value: ACCEPT
            - name: FELIX_IPV6SUPPORT
              value: "false"
            - name: FELIX_LOGSEVERITYSCREEN
              valueFrom:
                configMapKeyRef:
                  key: felix_log_level
                  name: calico-config
            - name: FELIX_HEALTHENABLED
              value: "true"
            image: calico/node:v3.8.2
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 6
              httpGet:
                host: localhost
                path: /liveness
                port: 9099
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            name: calico-node
            readinessProbe:
              exec:
                command:
                - /bin/calico-node
                - -bird-ready
                - -felix-ready
              failureThreshold: 3
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            resources:
              requests:
                cpu: 250m
            securityContext:
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /run/xtables.lock
              name: xtables-lock
            - mountPath: /var/run/calico
              name: var-run-calico
            - mountPath: /var/lib/calico
              name: var-lib-calico
            - mountPath: /var/run/nodeagent
              name: policysync
          dnsPolicy: ClusterFirst
          hostNetwork: true
          initContainers:
          - command:
            - /install-cni.sh
            env:
            - name: CNI_CONF_NAME
              value: 10-calico.conflist
            - name: CNI_NETWORK_CONFIG
              valueFrom:
                configMapKeyRef:
                  key: cni_network_config
                  name: calico-config
            - name: KUBERNETES_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CNI_MTU
              valueFrom:
                configMapKeyRef:
                  key: veth_mtu
                  name: calico-config
            - name: SLEEP
              value: "false"
            image: calico/cni:v3.8.2
            imagePullPolicy: IfNotPresent
            name: install-cni
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /host/opt/cni/bin
              name: cni-bin-dir
            - mountPath: /host/etc/cni/net.d
              name: cni-net-dir
          - image: calico/pod2daemon-flexvol:v3.8.2
            imagePullPolicy: IfNotPresent
            name: flexvol-driver
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /host/driver
              name: flexvol-driver-host
          nodeSelector:
            beta.kubernetes.io/os: linux
          priorityClassName: system-node-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: calico-node
          serviceAccountName: calico-node
          terminationGracePeriodSeconds: 0
          tolerations:
          - effect: NoSchedule
            operator: Exists
          - key: CriticalAddonsOnly
            operator: Exists
          - effect: NoExecute
            operator: Exists
          volumes:
          - hostPath:
              path: /lib/modules
              type: ""
            name: lib-modules
          - hostPath:
              path: /var/run/calico
              type: ""
            name: var-run-calico
          - hostPath:
              path: /var/lib/calico
              type: ""
            name: var-lib-calico
          - hostPath:
              path: /run/xtables.lock
              type: FileOrCreate
            name: xtables-lock
          - hostPath:
              path: /opt/cni/bin
              type: ""
            name: cni-bin-dir
          - hostPath:
              path: /etc/cni/net.d
              type: ""
            name: cni-net-dir
          - hostPath:
              path: /var/run/nodeagent
              type: DirectoryOrCreate
            name: policysync
          - hostPath:
              path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
              type: DirectoryOrCreate
            name: flexvol-driver-host
      updateStrategy:
        rollingUpdate:
          maxUnavailable: 1
        type: RollingUpdate
    status:
      currentNumberScheduled: 1
      desiredNumberScheduled: 1
      numberAvailable: 1
      numberMisscheduled: 0
      numberReady: 1
      observedGeneration: 1
      updatedNumberScheduled: 1
  - apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      annotations:
        deprecated.daemonset.template.generation: "1"
      creationTimestamp: "2020-04-27T20:34:17Z"
      generation: 1
      labels:
        k8s-app: kube-proxy
      name: kube-proxy
      namespace: kube-system
      resourceVersion: "500"
      selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy
      uid: 9e0b8f56-f15d-45a2-80d0-31234933690b
    spec:
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          k8s-app: kube-proxy
      template:
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kube-proxy
        spec:
          containers:
          - command:
            - /usr/local/bin/kube-proxy
            - --config=/var/lib/kube-proxy/config.conf
            - --hostname-override=$(NODE_NAME)
            env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            image: k8s.gcr.io/kube-proxy:v1.15.7
            imagePullPolicy: IfNotPresent
            name: kube-proxy
            resources: {}
            securityContext:
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /var/lib/kube-proxy
              name: kube-proxy
            - mountPath: /run/xtables.lock
              name: xtables-lock
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
          dnsPolicy: ClusterFirst
          hostNetwork: true
          nodeSelector:
            beta.kubernetes.io/os: linux
          priorityClassName: system-node-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: kube-proxy
          serviceAccountName: kube-proxy
          terminationGracePeriodSeconds: 30
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          - operator: Exists
          volumes:
          - configMap:
              defaultMode: 420
              name: kube-proxy
            name: kube-proxy
          - hostPath:
              path: /run/xtables.lock
              type: FileOrCreate
            name: xtables-lock
          - hostPath:
              path: /lib/modules
              type: ""
            name: lib-modules
      updateStrategy:
        rollingUpdate:
          maxUnavailable: 1
        type: RollingUpdate
    status:
      currentNumberScheduled: 1
      desiredNumberScheduled: 1
      numberAvailable: 1
      numberMisscheduled: 0
      numberReady: 1
      observedGeneration: 1
      updatedNumberScheduled: 1
  - apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      annotations:
        deprecated.daemonset.template.generation: "1"
      creationTimestamp: "2020-04-27T20:35:28Z"
      generation: 1
      labels:
        addonmanager.kubernetes.io/mode: Reconcile
        k8s-app: node-local-dns
        kubernetes.io/cluster-service: "true"
      name: node-local-dns
      namespace: kube-system
      resourceVersion: "854"
      selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/node-local-dns
      uid: e15f3924-aa24-4407-890f-f66c84e4e7a2
    spec:
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          k8s-app: node-local-dns
      template:
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: node-local-dns
        spec:
          containers:
          - args:
            - -localip
            - 169.254.20.10,10.96.0.10
            - -conf
            - /etc/Corefile
            - -upstreamsvc
            - kube-dns-upstream
            image: k8s.gcr.io/k8s-dns-node-cache:1.15.7
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 3
              httpGet:
                host: 169.254.20.10
                path: /health
                port: 8080
                scheme: HTTP
              initialDelaySeconds: 60
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 5
            name: node-cache
            ports:
            - containerPort: 53
              hostPort: 53
              name: dns
              protocol: UDP
            - containerPort: 53
              hostPort: 53
              name: dns-tcp
              protocol: TCP
            - containerPort: 9253
              hostPort: 9253
              name: metrics
              protocol: TCP
            resources:
              requests:
                cpu: 25m
                memory: 5Mi
            securityContext:
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /run/xtables.lock
              name: xtables-lock
            - mountPath: /etc/coredns
              name: config-volume
            - mountPath: /etc/kube-dns
              name: kube-dns-config
          dnsPolicy: Default
          hostNetwork: true
          priorityClassName: system-node-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: node-local-dns
          serviceAccountName: node-local-dns
          terminationGracePeriodSeconds: 30
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          volumes:
          - hostPath:
              path: /run/xtables.lock
              type: FileOrCreate
            name: xtables-lock
          - configMap:
              defaultMode: 420
              name: kube-dns
              optional: true
            name: kube-dns-config
          - configMap:
              defaultMode: 420
              items:
              - key: Corefile
                path: Corefile.base
              name: node-local-dns
            name: config-volume
      updateStrategy:
        rollingUpdate:
          maxUnavailable: 10%
        type: RollingUpdate
    status:
      currentNumberScheduled: 1
      desiredNumberScheduled: 1
      numberAvailable: 1
      numberMisscheduled: 0
      numberReady: 1
      observedGeneration: 1
      updatedNumberScheduled: 1
  metadata: {}
- items:
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      annotations:
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:34:31Z"
      generation: 2
      labels:
        k8s-app: calico-kube-controllers
      name: calico-kube-controllers
      namespace: kube-system
      resourceVersion: "2018"
      selfLink: /apis/apps/v1/namespaces/kube-system/deployments/calico-kube-controllers
      uid: 7b3ad329-42ac-4f5c-b982-1a382a91ccca
    spec:
      progressDeadlineSeconds: 600
      replicas: 1
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          k8s-app: calico-kube-controllers
      strategy:
        type: Recreate
      template:
        metadata:
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ""
          creationTimestamp: null
          labels:
            k8s-app: calico-kube-controllers
          name: calico-kube-controllers
          namespace: kube-system
        spec:
          containers:
          - env:
            - name: ENABLED_CONTROLLERS
              value: node
            - name: DATASTORE_TYPE
              value: kubernetes
            image: calico/kube-controllers:v3.8.2
            imagePullPolicy: IfNotPresent
            name: calico-kube-controllers
            readinessProbe:
              exec:
                command:
                - /usr/bin/check-status
                - -r
              failureThreshold: 3
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          nodeSelector:
            beta.kubernetes.io/os: linux
          priorityClassName: system-cluster-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: calico-kube-controllers
          serviceAccountName: calico-kube-controllers
          terminationGracePeriodSeconds: 30
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
    status:
      availableReplicas: 1
      conditions:
      - lastTransitionTime: "2020-04-27T20:34:54Z"
        lastUpdateTime: "2020-04-27T20:34:54Z"
        message: Deployment has minimum availability.
        reason: MinimumReplicasAvailable
        status: "True"
        type: Available
      - lastTransitionTime: "2020-04-27T20:34:35Z"
        lastUpdateTime: "2020-04-27T20:34:54Z"
        message: ReplicaSet "calico-kube-controllers-65b8787765" has successfully
          progressed.
        reason: NewReplicaSetAvailable
        status: "True"
        type: Progressing
      observedGeneration: 2
      readyReplicas: 1
      replicas: 1
      updatedReplicas: 1
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      annotations:
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:34:17Z"
      generation: 1
      labels:
        k8s-app: kube-dns
      name: coredns
      namespace: kube-system
      resourceVersion: "745"
      selfLink: /apis/apps/v1/namespaces/kube-system/deployments/coredns
      uid: 40ad49e0-7f0a-490c-b7d1-083faf79059f
    spec:
      progressDeadlineSeconds: 600
      replicas: 2
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          k8s-app: kube-dns
      strategy:
        rollingUpdate:
          maxSurge: 25%
          maxUnavailable: 1
        type: RollingUpdate
      template:
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kube-dns
        spec:
          containers:
          - args:
            - -conf
            - /etc/coredns/Corefile
            image: k8s.gcr.io/coredns:1.3.1
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 5
              httpGet:
                path: /health
                port: 8080
                scheme: HTTP
              initialDelaySeconds: 60
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 5
            name: coredns
            ports:
            - containerPort: 53
              name: dns
              protocol: UDP
            - containerPort: 53
              name: dns-tcp
              protocol: TCP
            - containerPort: 9153
              name: metrics
              protocol: TCP
            readinessProbe:
              failureThreshold: 3
              httpGet:
                path: /health
                port: 8080
                scheme: HTTP
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            resources:
              limits:
                memory: 170Mi
              requests:
                cpu: 100m
                memory: 70Mi
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                add:
                - NET_BIND_SERVICE
                drop:
                - all
              readOnlyRootFilesystem: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /etc/coredns
              name: config-volume
              readOnly: true
            - mountPath: /tmp
              name: tmp
          dnsPolicy: Default
          nodeSelector:
            beta.kubernetes.io/os: linux
          priorityClassName: system-cluster-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: coredns
          serviceAccountName: coredns
          terminationGracePeriodSeconds: 30
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          volumes:
          - emptyDir: {}
            name: tmp
          - configMap:
              defaultMode: 420
              items:
              - key: Corefile
                path: Corefile
              name: coredns
            name: config-volume
    status:
      availableReplicas: 2
      conditions:
      - lastTransitionTime: "2020-04-27T20:34:50Z"
        lastUpdateTime: "2020-04-27T20:34:50Z"
        message: Deployment has minimum availability.
        reason: MinimumReplicasAvailable
        status: "True"
        type: Available
      - lastTransitionTime: "2020-04-27T20:34:35Z"
        lastUpdateTime: "2020-04-27T20:35:06Z"
        message: ReplicaSet "coredns-5d4dd4b4db" has successfully progressed.
        reason: NewReplicaSetAvailable
        status: "True"
        type: Progressing
      observedGeneration: 1
      readyReplicas: 2
      replicas: 2
      updatedReplicas: 2
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      annotations:
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:35:44Z"
      generation: 2
      labels:
        k8s-app: kubernetes-dashboard
      name: kubernetes-dashboard
      namespace: kube-system
      resourceVersion: "2210"
      selfLink: /apis/apps/v1/namespaces/kube-system/deployments/kubernetes-dashboard
      uid: e8a80f24-557e-411d-9f55-5c2386bce605
    spec:
      progressDeadlineSeconds: 600
      replicas: 1
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          k8s-app: kubernetes-dashboard
      strategy:
        rollingUpdate:
          maxSurge: 25%
          maxUnavailable: 25%
        type: RollingUpdate
      template:
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kubernetes-dashboard
        spec:
          containers:
          - args:
            - --auto-generate-certificates
            image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 3
              httpGet:
                path: /
                port: 8443
                scheme: HTTPS
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 30
            name: kubernetes-dashboard
            ports:
            - containerPort: 8443
              protocol: TCP
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /certs
              name: kubernetes-dashboard-certs
            - mountPath: /tmp
              name: tmp-volume
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: kubernetes-dashboard
          serviceAccountName: kubernetes-dashboard
          terminationGracePeriodSeconds: 30
          tolerations:
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          volumes:
          - name: kubernetes-dashboard-certs
            secret:
              defaultMode: 420
              secretName: kubernetes-dashboard-certs
          - emptyDir: {}
            name: tmp-volume
    status:
      availableReplicas: 1
      conditions:
      - lastTransitionTime: "2020-04-27T20:35:49Z"
        lastUpdateTime: "2020-04-27T20:35:49Z"
        message: Deployment has minimum availability.
        reason: MinimumReplicasAvailable
        status: "True"
        type: Available
      - lastTransitionTime: "2020-04-27T20:35:44Z"
        lastUpdateTime: "2020-04-27T20:35:49Z"
        message: ReplicaSet "kubernetes-dashboard-7d75c474bb" has successfully progressed.
        reason: NewReplicaSetAvailable
        status: "True"
        type: Progressing
      observedGeneration: 2
      readyReplicas: 1
      replicas: 1
      updatedReplicas: 1
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      annotations:
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:35:47Z"
      generation: 2
      name: namespace-configuration-operator
      namespace: kube-system
      resourceVersion: "2223"
      selfLink: /apis/apps/v1/namespaces/kube-system/deployments/namespace-configuration-operator
      uid: 90a9a6de-da61-46a1-9d08-174f551e173e
    spec:
      progressDeadlineSeconds: 600
      replicas: 1
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          name: namespace-configuration-operator
      strategy:
        rollingUpdate:
          maxSurge: 25%
          maxUnavailable: 25%
        type: RollingUpdate
      template:
        metadata:
          creationTimestamp: null
          labels:
            name: namespace-configuration-operator
        spec:
          containers:
          - command:
            - namespace-configuration-operator
            env:
            - name: WATCH_NAMESPACE
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: OPERATOR_NAME
              value: namespace-configuration-operator
            image: quay.io/redhat-cop/namespace-configuration-operator:v0.1.0
            imagePullPolicy: IfNotPresent
            name: namespace-configuration-operator
            resources:
              limits:
                cpu: 100m
                memory: 256Mi
              requests:
                cpu: 10m
                memory: 20Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: namespace-configuration-operator
          serviceAccountName: namespace-configuration-operator
          terminationGracePeriodSeconds: 30
    status:
      availableReplicas: 1
      conditions:
      - lastTransitionTime: "2020-04-27T20:35:53Z"
        lastUpdateTime: "2020-04-27T20:35:53Z"
        message: Deployment has minimum availability.
        reason: MinimumReplicasAvailable
        status: "True"
        type: Available
      - lastTransitionTime: "2020-04-27T20:35:47Z"
        lastUpdateTime: "2020-04-27T20:35:53Z"
        message: ReplicaSet "namespace-configuration-operator-6f748b789c" has successfully
          progressed.
        reason: NewReplicaSetAvailable
        status: "True"
        type: Progressing
      observedGeneration: 2
      readyReplicas: 1
      replicas: 1
      updatedReplicas: 1
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      annotations:
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:35:48Z"
      generation: 2
      name: platform-operator
      namespace: kube-system
      resourceVersion: "2233"
      selfLink: /apis/apps/v1/namespaces/kube-system/deployments/platform-operator
      uid: a60fb529-a6ba-4edb-ac56-220a78ed8dc6
    spec:
      progressDeadlineSeconds: 600
      replicas: 1
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          name: platform-operator
      strategy:
        rollingUpdate:
          maxSurge: 25%
          maxUnavailable: 25%
        type: RollingUpdate
      template:
        metadata:
          creationTimestamp: null
          labels:
            name: platform-operator
        spec:
          containers:
          - image: docker.io/moshloop/platform-operator:0.1
            imagePullPolicy: IfNotPresent
            name: platform-operator
            resources:
              limits:
                cpu: 100m
                memory: 256Mi
              requests:
                cpu: 10m
                memory: 20Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: platform-operator
          serviceAccountName: platform-operator
          terminationGracePeriodSeconds: 30
    status:
      availableReplicas: 1
      conditions:
      - lastTransitionTime: "2020-04-27T20:35:55Z"
        lastUpdateTime: "2020-04-27T20:35:55Z"
        message: Deployment has minimum availability.
        reason: MinimumReplicasAvailable
        status: "True"
        type: Available
      - lastTransitionTime: "2020-04-27T20:35:48Z"
        lastUpdateTime: "2020-04-27T20:35:55Z"
        message: ReplicaSet "platform-operator-995b7fb78" has successfully progressed.
        reason: NewReplicaSetAvailable
        status: "True"
        type: Progressing
      observedGeneration: 2
      readyReplicas: 1
      replicas: 1
      updatedReplicas: 1
  metadata: {}
- items:
  - apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      annotations:
        deployment.kubernetes.io/desired-replicas: "1"
        deployment.kubernetes.io/max-replicas: "1"
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:34:35Z"
      generation: 1
      labels:
        k8s-app: calico-kube-controllers
        pod-template-hash: 65b8787765
      name: calico-kube-controllers-65b8787765
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: Deployment
        name: calico-kube-controllers
        uid: 7b3ad329-42ac-4f5c-b982-1a382a91ccca
      resourceVersion: "669"
      selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/calico-kube-controllers-65b8787765
      uid: 8492a565-f513-4f1a-aace-c607bc964343
    spec:
      replicas: 1
      selector:
        matchLabels:
          k8s-app: calico-kube-controllers
          pod-template-hash: 65b8787765
      template:
        metadata:
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ""
          creationTimestamp: null
          labels:
            k8s-app: calico-kube-controllers
            pod-template-hash: 65b8787765
          name: calico-kube-controllers
          namespace: kube-system
        spec:
          containers:
          - env:
            - name: ENABLED_CONTROLLERS
              value: node
            - name: DATASTORE_TYPE
              value: kubernetes
            image: calico/kube-controllers:v3.8.2
            imagePullPolicy: IfNotPresent
            name: calico-kube-controllers
            readinessProbe:
              exec:
                command:
                - /usr/bin/check-status
                - -r
              failureThreshold: 3
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          nodeSelector:
            beta.kubernetes.io/os: linux
          priorityClassName: system-cluster-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: calico-kube-controllers
          serviceAccountName: calico-kube-controllers
          terminationGracePeriodSeconds: 30
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
    status:
      availableReplicas: 1
      fullyLabeledReplicas: 1
      observedGeneration: 1
      readyReplicas: 1
      replicas: 1
  - apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      annotations:
        deployment.kubernetes.io/desired-replicas: "2"
        deployment.kubernetes.io/max-replicas: "3"
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:34:35Z"
      generation: 1
      labels:
        k8s-app: kube-dns
        pod-template-hash: 5d4dd4b4db
      name: coredns-5d4dd4b4db
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: Deployment
        name: coredns
        uid: 40ad49e0-7f0a-490c-b7d1-083faf79059f
      resourceVersion: "743"
      selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/coredns-5d4dd4b4db
      uid: 02a4e24b-1af1-4f32-ba89-fbd10e813ca9
    spec:
      replicas: 2
      selector:
        matchLabels:
          k8s-app: kube-dns
          pod-template-hash: 5d4dd4b4db
      template:
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kube-dns
            pod-template-hash: 5d4dd4b4db
        spec:
          containers:
          - args:
            - -conf
            - /etc/coredns/Corefile
            image: k8s.gcr.io/coredns:1.3.1
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 5
              httpGet:
                path: /health
                port: 8080
                scheme: HTTP
              initialDelaySeconds: 60
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 5
            name: coredns
            ports:
            - containerPort: 53
              name: dns
              protocol: UDP
            - containerPort: 53
              name: dns-tcp
              protocol: TCP
            - containerPort: 9153
              name: metrics
              protocol: TCP
            readinessProbe:
              failureThreshold: 3
              httpGet:
                path: /health
                port: 8080
                scheme: HTTP
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            resources:
              limits:
                memory: 170Mi
              requests:
                cpu: 100m
                memory: 70Mi
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                add:
                - NET_BIND_SERVICE
                drop:
                - all
              readOnlyRootFilesystem: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /etc/coredns
              name: config-volume
              readOnly: true
            - mountPath: /tmp
              name: tmp
          dnsPolicy: Default
          nodeSelector:
            beta.kubernetes.io/os: linux
          priorityClassName: system-cluster-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: coredns
          serviceAccountName: coredns
          terminationGracePeriodSeconds: 30
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          volumes:
          - emptyDir: {}
            name: tmp
          - configMap:
              defaultMode: 420
              items:
              - key: Corefile
                path: Corefile
              name: coredns
            name: config-volume
    status:
      availableReplicas: 2
      fullyLabeledReplicas: 2
      observedGeneration: 1
      readyReplicas: 2
      replicas: 2
  - apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      annotations:
        deployment.kubernetes.io/desired-replicas: "1"
        deployment.kubernetes.io/max-replicas: "2"
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:35:44Z"
      generation: 1
      labels:
        k8s-app: kubernetes-dashboard
        pod-template-hash: 7d75c474bb
      name: kubernetes-dashboard-7d75c474bb
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: Deployment
        name: kubernetes-dashboard
        uid: e8a80f24-557e-411d-9f55-5c2386bce605
      resourceVersion: "1173"
      selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/kubernetes-dashboard-7d75c474bb
      uid: 693b795b-0fa7-4c83-be3d-e14db10528a6
    spec:
      replicas: 1
      selector:
        matchLabels:
          k8s-app: kubernetes-dashboard
          pod-template-hash: 7d75c474bb
      template:
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kubernetes-dashboard
            pod-template-hash: 7d75c474bb
        spec:
          containers:
          - args:
            - --auto-generate-certificates
            image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 3
              httpGet:
                path: /
                port: 8443
                scheme: HTTPS
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 30
            name: kubernetes-dashboard
            ports:
            - containerPort: 8443
              protocol: TCP
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /certs
              name: kubernetes-dashboard-certs
            - mountPath: /tmp
              name: tmp-volume
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: kubernetes-dashboard
          serviceAccountName: kubernetes-dashboard
          terminationGracePeriodSeconds: 30
          tolerations:
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          volumes:
          - name: kubernetes-dashboard-certs
            secret:
              defaultMode: 420
              secretName: kubernetes-dashboard-certs
          - emptyDir: {}
            name: tmp-volume
    status:
      availableReplicas: 1
      fullyLabeledReplicas: 1
      observedGeneration: 1
      readyReplicas: 1
      replicas: 1
  - apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      annotations:
        deployment.kubernetes.io/desired-replicas: "1"
        deployment.kubernetes.io/max-replicas: "2"
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:35:47Z"
      generation: 1
      labels:
        name: namespace-configuration-operator
        pod-template-hash: 6f748b789c
      name: namespace-configuration-operator-6f748b789c
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: Deployment
        name: namespace-configuration-operator
        uid: 90a9a6de-da61-46a1-9d08-174f551e173e
      resourceVersion: "1269"
      selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/namespace-configuration-operator-6f748b789c
      uid: c2b7e9e4-fd04-4c2b-b1ce-24d16a201646
    spec:
      replicas: 1
      selector:
        matchLabels:
          name: namespace-configuration-operator
          pod-template-hash: 6f748b789c
      template:
        metadata:
          creationTimestamp: null
          labels:
            name: namespace-configuration-operator
            pod-template-hash: 6f748b789c
        spec:
          containers:
          - command:
            - namespace-configuration-operator
            env:
            - name: WATCH_NAMESPACE
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: OPERATOR_NAME
              value: namespace-configuration-operator
            image: quay.io/redhat-cop/namespace-configuration-operator:v0.1.0
            imagePullPolicy: IfNotPresent
            name: namespace-configuration-operator
            resources:
              limits:
                cpu: 100m
                memory: 256Mi
              requests:
                cpu: 10m
                memory: 20Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: namespace-configuration-operator
          serviceAccountName: namespace-configuration-operator
          terminationGracePeriodSeconds: 30
    status:
      availableReplicas: 1
      fullyLabeledReplicas: 1
      observedGeneration: 1
      readyReplicas: 1
      replicas: 1
  - apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      annotations:
        deployment.kubernetes.io/desired-replicas: "1"
        deployment.kubernetes.io/max-replicas: "2"
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:35:48Z"
      generation: 1
      labels:
        name: platform-operator
        pod-template-hash: 995b7fb78
      name: platform-operator-995b7fb78
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: Deployment
        name: platform-operator
        uid: a60fb529-a6ba-4edb-ac56-220a78ed8dc6
      resourceVersion: "1334"
      selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/platform-operator-995b7fb78
      uid: 711594c6-dc79-4eaf-a72f-03fb8e1244d4
    spec:
      replicas: 1
      selector:
        matchLabels:
          name: platform-operator
          pod-template-hash: 995b7fb78
      template:
        metadata:
          creationTimestamp: null
          labels:
            name: platform-operator
            pod-template-hash: 995b7fb78
        spec:
          containers:
          - image: docker.io/moshloop/platform-operator:0.1
            imagePullPolicy: IfNotPresent
            name: platform-operator
            resources:
              limits:
                cpu: 100m
                memory: 256Mi
              requests:
                cpu: 10m
                memory: 20Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: platform-operator
          serviceAccountName: platform-operator
          terminationGracePeriodSeconds: 30
    status:
      availableReplicas: 1
      fullyLabeledReplicas: 1
      observedGeneration: 1
      readyReplicas: 1
      replicas: 1
  metadata: {}
- items:
  - apiVersion: cert-manager.io/v1alpha2
    kind: CertificateRequest
    metadata:
      annotations:
        cert-manager.io/certificate-name: dashboard-tls
        cert-manager.io/private-key-secret-name: dashboard-tls
      creationTimestamp: "2020-04-27T20:35:46Z"
      generation: 1
      name: dashboard-tls-1828523392
      namespace: kube-system
      ownerReferences:
      - apiVersion: cert-manager.io/v1alpha2
        blockOwnerDeletion: true
        controller: true
        kind: Certificate
        name: dashboard-tls
        uid: e2ba4d85-5b1c-4ba5-91c7-662254c7627c
      resourceVersion: "1083"
      selfLink: /apis/cert-manager.io/v1alpha2/namespaces/kube-system/certificaterequests/dashboard-tls-1828523392
      uid: e951d1fd-9336-4570-9d3b-915161d3cc4a
    spec:
      csr: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ2xEQ0NBWHdDQVFBd0Z6RVZNQk1HQTFVRUNoTU1ZMlZ5ZEMxdFlXNWhaMlZ5TUlJQklqQU5CZ2txaGtpRwo5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBeGNhUW9vTjZjbVU2KzVFLzdyZVpYYUgySTd4eC9FcFlGbmQyCld5NExIbDJIVkZGRG14UHZVMEFmSmtmUERsMnI2MHZVcXV0NkFDdGdWQWxMNVFJcVFhaUZia2VRTG83VnFCemEKcWpPRWpFRmdBN0JwYVFMY1locEFhTlRDRnlqakFINzkrVXNDY0YrOElTbXQvaW4waFdGdW1HK2szditaRklycworMWdtY1h3VFVidW1SYzhxYyswWjR3eVc5WTYzL1M4eDJiOXNsTlAxZHJHYWVYcVU2UWZSVlpVelNiUDREMDRTClorNjE4cERBUUcxTUlmWjdobHBwUGdnclhaUVdQVkcrZ2tGTmVTdTBuR0JqREhqVUNOazU3dnBMWk54dHk3NkIKOUI0bkNwSW1iZFMrWUV0NjdTOE1iR1JRSHhkOU5ob2piSkJpUlgxNUI2ZVB4ci84MlFJREFRQUJvRGd3TmdZSgpLb1pJaHZjTkFRa09NU2t3SnpBbEJnTlZIUkVFSGpBY2docGtZWE5vWW05aGNtUXVNVEkzTGpBdU1DNHhMbTVwCmNDNXBiekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBWmd2U2h2QUlOaGhoTWJnYVh5amdDRnZqOURSRWY3TFoKMWpML0tXK1AxZDVjdlZXSmVsK0FMTmtLaTZhLzliTExGTnpMTnJSMjJDZmhVajMvYVNFV25ZNTQ4Z1VTeCsydgpRM3pyb0xwYlI5RHJnbnphc0tVNytTZlZQejk0eVl4MXNTWU1SSjlVbzdsdWJINmFXeXFNRlZ1TmFHRmdnUktFCm5pL1QrR2FseEJTdDVVQVZyZ1phdnRrQUdBZ0I1UjdjbEVxanVhZTI3UUJaVkpRcmZsSFdDRXVwa1MwejYxSUcKQWlCSzNPUmVRUnBpL3d2cHBjcUtlZUloU3RzMEFZSDZRQ2ZyY3duUFhYMFllbHlNTzVvYlJGaHZSa0Ixa2VheApNYXhHMHBpTWFBaEwvcnY2SjlWVjFvUUJVRHFtZjA2YkZWblI5VkhCWkZoQlZmZHlaL2sxcVE9PQotLS0tLUVORCBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0K
      issuerRef:
        group: cert-manager.io
        kind: ClusterIssuer
        name: ingress-ca
    status:
      ca: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMwakNDQWJxZ0F3SUJBZ0lJTTJrUWVscjYycTh3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2FXNW5jbVZ6Y3kxallUQWVGdzB5TURBME1qY3lNREU0TURaYUZ3MHlNVEEwTWpZeU1ETXpNRFphTUJVeApFekFSQmdOVkJBTVRDbWx1WjNKbGMzTXRZMkV3Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUR6SXdlRnFIYktoRjNCWUdDaTFCOXAwNmNOVU1Hc2MyMm9lWVcvYmlqZmkvMWRRR3ViOHVBRnNPVG4KMTBHc0EvbzZxd25MS1BtT2k1U3Y3MzVlRGpPMThxSXMwcUhrdkVmbG8xVXdKZGw2Y3ZVU1ZDYTNlbStsU28xUwpEY1dxeDJuRlFjUVgxOEluc2dLYnhQZGY5UE5uQkM4QnE0L2ZPQTRoM1BBNTJEUHQxVGFCODNZZFhMdVNnMzZUCmUwTWI3ZnNadDRGZDMxWTArK0kxQklLc0dWWk0rejQxVlYrcWlucFpsTkZhQ0F4d2VwTnJPMGhXOUoxRlh0akYKTSsrYVR5bHNCQlJmV1FRcFFIZXAvME4vSzBZNDVPeURQVmdJNWp5a2d1VzM0aFZJRGYya3ViYldXMmJObVpIUAorVUI1MkZxSFpRemxWU1ZYMWhiSVI2Wm5lZzMzQWdNQkFBR2pKakFrTUE0R0ExVWREd0VCL3dRRUF3SUNwREFTCkJnTlZIUk1CQWY4RUNEQUdBUUgvQWdFQU1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQ2Ura0VsWlZrZEdvVGcKVkVOQkhERGJYMkY5N0FmRDFMcjlaWTVlMHFORVJlZnpvSElJdGJrN00reXk3QTRLNzJkcHBzdlYya0ZtMXhZcApnd1o1OGZKWGtSSk5XQjlOMjNIcHVJUXdzZmVFTkpBYk8yL1UrWEZVdnBZMitaazQ5bE5oUEZWZFBjcS9nTXdSCkQ2Y2hWckF6L3NvLzh2V0h3WDREOTBQL01vd0dWQ2F4aENEWVZDRGt1WERKeGdwdnEvSDlQaXU3YlNYUjd1TVEKb3Z4V0QxN1BuN2xmdXFSbitjT2xmMVdmTXlQUmJXNHY1bGVjb0J0SGdZUkhsMG9PVU5SVXdJTHIydkkrczkzTwpjMmVPQ3dLZU9iWGllSUU3RjRGVWs2cGNUdjFBbDRjRXlzK2IyVWFxek96SndlY3J0ZUh5Z2ZTYjBWVE1mL3BwCmEzd3hZV0VDCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
      certificate: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURFakNDQWZxZ0F3SUJBZ0lRTEFFaFU5aWloSHhCaHkyQ3IwWmkyekFOQmdrcWhraUc5dzBCQVFzRkFEQVYKTVJNd0VRWURWUVFERXdwcGJtZHlaWE56TFdOaE1CNFhEVEl3TURReU56SXdNelUwTmxvWERUSXdNRGN5TmpJdwpNelUwTmxvd0Z6RVZNQk1HQTFVRUNoTU1ZMlZ5ZEMxdFlXNWhaMlZ5TUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGCkFBT0NBUThBTUlJQkNnS0NBUUVBeGNhUW9vTjZjbVU2KzVFLzdyZVpYYUgySTd4eC9FcFlGbmQyV3k0TEhsMkgKVkZGRG14UHZVMEFmSmtmUERsMnI2MHZVcXV0NkFDdGdWQWxMNVFJcVFhaUZia2VRTG83VnFCemFxak9FakVGZwpBN0JwYVFMY1locEFhTlRDRnlqakFINzkrVXNDY0YrOElTbXQvaW4waFdGdW1HK2szditaRklycysxZ21jWHdUClVidW1SYzhxYyswWjR3eVc5WTYzL1M4eDJiOXNsTlAxZHJHYWVYcVU2UWZSVlpVelNiUDREMDRTWis2MThwREEKUUcxTUlmWjdobHBwUGdnclhaUVdQVkcrZ2tGTmVTdTBuR0JqREhqVUNOazU3dnBMWk54dHk3NkI5QjRuQ3BJbQpiZFMrWUV0NjdTOE1iR1JRSHhkOU5ob2piSkJpUlgxNUI2ZVB4ci84MlFJREFRQUJvMXd3V2pBT0JnTlZIUThCCkFmOEVCQU1DQmFBd0V3WURWUjBsQkF3d0NnWUlLd1lCQlFVSEF3RXdEQVlEVlIwVEFRSC9CQUl3QURBbEJnTlYKSFJFRUhqQWNnaHBrWVhOb1ltOWhjbVF1TVRJM0xqQXVNQzR4TG01cGNDNXBiekFOQmdrcWhraUc5dzBCQVFzRgpBQU9DQVFFQTYvVkxicWc1VmVPYXF5QzRJbXluU0ZYaGVpUCtpcWR3di9IMzNmZ3hId1BCKzd0Q3AxalN6WlllClV2aUJzalpoKzQ2cExiV2JiSFZRR2U3dnRkVElQMUd2LzArSFgydGcxcFpKclpxWDR2ZVhvK2dQR0Y3Qzk3TysKN01Cdy91UE9NMHl0MDM5M0hNdlZobEVHdjdvY2QxaTJTT3FFdFZKQTQvWk5Rb3U3U0IwaStJY2N3cVptaGlVZwozbjA2bVhkaUxnaHUxdmIwUkNzZEhMK1ZNMXJqS3pEU0hpS1BHVUxZbkorQXk3NmpBS0lydEpyYmJDVnlLRVVGCnJ5QUh2UXpwTnUwbHdkekVYRkY4WHQ2TWxUWTI4dkFXYjUwMkt2QVNMSktYaVpMRE9SdDIxU1NNbnV3UGZyTE8KWGpQR0ZGVnVnUGpialkxOVBDQTBZRmdYTUxTd2VBPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
      conditions:
      - lastTransitionTime: "2020-04-27T20:35:46Z"
        message: Certificate fetched from issuer successfully
        reason: Issued
        status: "True"
        type: Ready
  metadata: {}
- items:
  - apiVersion: cert-manager.io/v1alpha2
    kind: Certificate
    metadata:
      creationTimestamp: "2020-04-27T20:35:45Z"
      generation: 1
      name: dashboard-tls
      namespace: kube-system
      ownerReferences:
      - apiVersion: extensions/v1beta1
        blockOwnerDeletion: true
        controller: true
        kind: Ingress
        name: dashboard-ing
        uid: 96daeb72-df4c-4caa-8faf-b4b895a90219
      resourceVersion: "1089"
      selfLink: /apis/cert-manager.io/v1alpha2/namespaces/kube-system/certificates/dashboard-tls
      uid: e2ba4d85-5b1c-4ba5-91c7-662254c7627c
    spec:
      dnsNames:
      - dashboard.127.0.0.1.nip.io
      issuerRef:
        group: cert-manager.io
        kind: ClusterIssuer
        name: ingress-ca
      secretName: dashboard-tls
    status:
      conditions:
      - lastTransitionTime: "2020-04-27T20:35:46Z"
        message: Certificate is up to date and has not expired
        reason: Ready
        status: "True"
        type: Ready
      notAfter: "2020-07-26T20:35:46Z"
  metadata: {}
- items:
  - apiVersion: extensions/v1beta1
    kind: DaemonSet
    metadata:
      creationTimestamp: "2020-04-27T20:34:30Z"
      generation: 1
      labels:
        k8s-app: calico-node
      name: calico-node
      namespace: kube-system
      resourceVersion: "677"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/calico-node
      uid: 7bf1cc4a-c741-466f-9ab4-ed1d4fb6d471
    spec:
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          k8s-app: calico-node
      template:
        metadata:
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ""
          creationTimestamp: null
          labels:
            k8s-app: calico-node
        spec:
          containers:
          - env:
            - name: FELIX_IGNORELOOSERPF
              value: "true"
            - name: DATASTORE_TYPE
              value: kubernetes
            - name: WAIT_FOR_DATASTORE
              value: "true"
            - name: NODENAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CALICO_NETWORKING_BACKEND
              valueFrom:
                configMapKeyRef:
                  key: calico_backend
                  name: calico-config
            - name: CLUSTER_TYPE
              valueFrom:
                configMapKeyRef:
                  key: cluster_type
                  name: calico-config
            - name: IP
              value: autodetect
            - name: CALICO_IPV4POOL_IPIP
              valueFrom:
                configMapKeyRef:
                  key: ipv4pool_ipip
                  name: calico-config
            - name: FELIX_IPINIPMTU
              valueFrom:
                configMapKeyRef:
                  key: veth_mtu
                  name: calico-config
            - name: CALICO_IPV4POOL_CIDR
              valueFrom:
                configMapKeyRef:
                  key: cluster_cidr
                  name: calico-config
            - name: CALICO_DISABLE_FILE_LOGGING
              value: "true"
            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
              value: ACCEPT
            - name: FELIX_IPV6SUPPORT
              value: "false"
            - name: FELIX_LOGSEVERITYSCREEN
              valueFrom:
                configMapKeyRef:
                  key: felix_log_level
                  name: calico-config
            - name: FELIX_HEALTHENABLED
              value: "true"
            image: calico/node:v3.8.2
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 6
              httpGet:
                host: localhost
                path: /liveness
                port: 9099
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            name: calico-node
            readinessProbe:
              exec:
                command:
                - /bin/calico-node
                - -bird-ready
                - -felix-ready
              failureThreshold: 3
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            resources:
              requests:
                cpu: 250m
            securityContext:
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /run/xtables.lock
              name: xtables-lock
            - mountPath: /var/run/calico
              name: var-run-calico
            - mountPath: /var/lib/calico
              name: var-lib-calico
            - mountPath: /var/run/nodeagent
              name: policysync
          dnsPolicy: ClusterFirst
          hostNetwork: true
          initContainers:
          - command:
            - /install-cni.sh
            env:
            - name: CNI_CONF_NAME
              value: 10-calico.conflist
            - name: CNI_NETWORK_CONFIG
              valueFrom:
                configMapKeyRef:
                  key: cni_network_config
                  name: calico-config
            - name: KUBERNETES_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CNI_MTU
              valueFrom:
                configMapKeyRef:
                  key: veth_mtu
                  name: calico-config
            - name: SLEEP
              value: "false"
            image: calico/cni:v3.8.2
            imagePullPolicy: IfNotPresent
            name: install-cni
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /host/opt/cni/bin
              name: cni-bin-dir
            - mountPath: /host/etc/cni/net.d
              name: cni-net-dir
          - image: calico/pod2daemon-flexvol:v3.8.2
            imagePullPolicy: IfNotPresent
            name: flexvol-driver
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /host/driver
              name: flexvol-driver-host
          nodeSelector:
            beta.kubernetes.io/os: linux
          priorityClassName: system-node-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: calico-node
          serviceAccountName: calico-node
          terminationGracePeriodSeconds: 0
          tolerations:
          - effect: NoSchedule
            operator: Exists
          - key: CriticalAddonsOnly
            operator: Exists
          - effect: NoExecute
            operator: Exists
          volumes:
          - hostPath:
              path: /lib/modules
              type: ""
            name: lib-modules
          - hostPath:
              path: /var/run/calico
              type: ""
            name: var-run-calico
          - hostPath:
              path: /var/lib/calico
              type: ""
            name: var-lib-calico
          - hostPath:
              path: /run/xtables.lock
              type: FileOrCreate
            name: xtables-lock
          - hostPath:
              path: /opt/cni/bin
              type: ""
            name: cni-bin-dir
          - hostPath:
              path: /etc/cni/net.d
              type: ""
            name: cni-net-dir
          - hostPath:
              path: /var/run/nodeagent
              type: DirectoryOrCreate
            name: policysync
          - hostPath:
              path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
              type: DirectoryOrCreate
            name: flexvol-driver-host
      templateGeneration: 1
      updateStrategy:
        rollingUpdate:
          maxUnavailable: 1
        type: RollingUpdate
    status:
      currentNumberScheduled: 1
      desiredNumberScheduled: 1
      numberAvailable: 1
      numberMisscheduled: 0
      numberReady: 1
      observedGeneration: 1
      updatedNumberScheduled: 1
  - apiVersion: extensions/v1beta1
    kind: DaemonSet
    metadata:
      creationTimestamp: "2020-04-27T20:34:17Z"
      generation: 1
      labels:
        k8s-app: kube-proxy
      name: kube-proxy
      namespace: kube-system
      resourceVersion: "500"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/kube-proxy
      uid: 9e0b8f56-f15d-45a2-80d0-31234933690b
    spec:
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          k8s-app: kube-proxy
      template:
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kube-proxy
        spec:
          containers:
          - command:
            - /usr/local/bin/kube-proxy
            - --config=/var/lib/kube-proxy/config.conf
            - --hostname-override=$(NODE_NAME)
            env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            image: k8s.gcr.io/kube-proxy:v1.15.7
            imagePullPolicy: IfNotPresent
            name: kube-proxy
            resources: {}
            securityContext:
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /var/lib/kube-proxy
              name: kube-proxy
            - mountPath: /run/xtables.lock
              name: xtables-lock
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
          dnsPolicy: ClusterFirst
          hostNetwork: true
          nodeSelector:
            beta.kubernetes.io/os: linux
          priorityClassName: system-node-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: kube-proxy
          serviceAccountName: kube-proxy
          terminationGracePeriodSeconds: 30
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          - operator: Exists
          volumes:
          - configMap:
              defaultMode: 420
              name: kube-proxy
            name: kube-proxy
          - hostPath:
              path: /run/xtables.lock
              type: FileOrCreate
            name: xtables-lock
          - hostPath:
              path: /lib/modules
              type: ""
            name: lib-modules
      templateGeneration: 1
      updateStrategy:
        rollingUpdate:
          maxUnavailable: 1
        type: RollingUpdate
    status:
      currentNumberScheduled: 1
      desiredNumberScheduled: 1
      numberAvailable: 1
      numberMisscheduled: 0
      numberReady: 1
      observedGeneration: 1
      updatedNumberScheduled: 1
  - apiVersion: extensions/v1beta1
    kind: DaemonSet
    metadata:
      creationTimestamp: "2020-04-27T20:35:28Z"
      generation: 1
      labels:
        addonmanager.kubernetes.io/mode: Reconcile
        k8s-app: node-local-dns
        kubernetes.io/cluster-service: "true"
      name: node-local-dns
      namespace: kube-system
      resourceVersion: "854"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/node-local-dns
      uid: e15f3924-aa24-4407-890f-f66c84e4e7a2
    spec:
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          k8s-app: node-local-dns
      template:
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: node-local-dns
        spec:
          containers:
          - args:
            - -localip
            - 169.254.20.10,10.96.0.10
            - -conf
            - /etc/Corefile
            - -upstreamsvc
            - kube-dns-upstream
            image: k8s.gcr.io/k8s-dns-node-cache:1.15.7
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 3
              httpGet:
                host: 169.254.20.10
                path: /health
                port: 8080
                scheme: HTTP
              initialDelaySeconds: 60
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 5
            name: node-cache
            ports:
            - containerPort: 53
              hostPort: 53
              name: dns
              protocol: UDP
            - containerPort: 53
              hostPort: 53
              name: dns-tcp
              protocol: TCP
            - containerPort: 9253
              hostPort: 9253
              name: metrics
              protocol: TCP
            resources:
              requests:
                cpu: 25m
                memory: 5Mi
            securityContext:
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /run/xtables.lock
              name: xtables-lock
            - mountPath: /etc/coredns
              name: config-volume
            - mountPath: /etc/kube-dns
              name: kube-dns-config
          dnsPolicy: Default
          hostNetwork: true
          priorityClassName: system-node-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: node-local-dns
          serviceAccountName: node-local-dns
          terminationGracePeriodSeconds: 30
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          volumes:
          - hostPath:
              path: /run/xtables.lock
              type: FileOrCreate
            name: xtables-lock
          - configMap:
              defaultMode: 420
              name: kube-dns
              optional: true
            name: kube-dns-config
          - configMap:
              defaultMode: 420
              items:
              - key: Corefile
                path: Corefile.base
              name: node-local-dns
            name: config-volume
      templateGeneration: 1
      updateStrategy:
        rollingUpdate:
          maxUnavailable: 10%
        type: RollingUpdate
    status:
      currentNumberScheduled: 1
      desiredNumberScheduled: 1
      numberAvailable: 1
      numberMisscheduled: 0
      numberReady: 1
      observedGeneration: 1
      updatedNumberScheduled: 1
  metadata: {}
- items:
  - apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      annotations:
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:34:31Z"
      generation: 2
      labels:
        k8s-app: calico-kube-controllers
      name: calico-kube-controllers
      namespace: kube-system
      resourceVersion: "2018"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/calico-kube-controllers
      uid: 7b3ad329-42ac-4f5c-b982-1a382a91ccca
    spec:
      progressDeadlineSeconds: 600
      replicas: 1
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          k8s-app: calico-kube-controllers
      strategy:
        type: Recreate
      template:
        metadata:
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ""
          creationTimestamp: null
          labels:
            k8s-app: calico-kube-controllers
          name: calico-kube-controllers
          namespace: kube-system
        spec:
          containers:
          - env:
            - name: ENABLED_CONTROLLERS
              value: node
            - name: DATASTORE_TYPE
              value: kubernetes
            image: calico/kube-controllers:v3.8.2
            imagePullPolicy: IfNotPresent
            name: calico-kube-controllers
            readinessProbe:
              exec:
                command:
                - /usr/bin/check-status
                - -r
              failureThreshold: 3
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          nodeSelector:
            beta.kubernetes.io/os: linux
          priorityClassName: system-cluster-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: calico-kube-controllers
          serviceAccountName: calico-kube-controllers
          terminationGracePeriodSeconds: 30
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
    status:
      availableReplicas: 1
      conditions:
      - lastTransitionTime: "2020-04-27T20:34:54Z"
        lastUpdateTime: "2020-04-27T20:34:54Z"
        message: Deployment has minimum availability.
        reason: MinimumReplicasAvailable
        status: "True"
        type: Available
      - lastTransitionTime: "2020-04-27T20:34:35Z"
        lastUpdateTime: "2020-04-27T20:34:54Z"
        message: ReplicaSet "calico-kube-controllers-65b8787765" has successfully
          progressed.
        reason: NewReplicaSetAvailable
        status: "True"
        type: Progressing
      observedGeneration: 2
      readyReplicas: 1
      replicas: 1
      updatedReplicas: 1
  - apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      annotations:
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:34:17Z"
      generation: 1
      labels:
        k8s-app: kube-dns
      name: coredns
      namespace: kube-system
      resourceVersion: "745"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/coredns
      uid: 40ad49e0-7f0a-490c-b7d1-083faf79059f
    spec:
      progressDeadlineSeconds: 600
      replicas: 2
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          k8s-app: kube-dns
      strategy:
        rollingUpdate:
          maxSurge: 25%
          maxUnavailable: 1
        type: RollingUpdate
      template:
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kube-dns
        spec:
          containers:
          - args:
            - -conf
            - /etc/coredns/Corefile
            image: k8s.gcr.io/coredns:1.3.1
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 5
              httpGet:
                path: /health
                port: 8080
                scheme: HTTP
              initialDelaySeconds: 60
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 5
            name: coredns
            ports:
            - containerPort: 53
              name: dns
              protocol: UDP
            - containerPort: 53
              name: dns-tcp
              protocol: TCP
            - containerPort: 9153
              name: metrics
              protocol: TCP
            readinessProbe:
              failureThreshold: 3
              httpGet:
                path: /health
                port: 8080
                scheme: HTTP
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            resources:
              limits:
                memory: 170Mi
              requests:
                cpu: 100m
                memory: 70Mi
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                add:
                - NET_BIND_SERVICE
                drop:
                - all
              readOnlyRootFilesystem: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /etc/coredns
              name: config-volume
              readOnly: true
            - mountPath: /tmp
              name: tmp
          dnsPolicy: Default
          nodeSelector:
            beta.kubernetes.io/os: linux
          priorityClassName: system-cluster-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: coredns
          serviceAccountName: coredns
          terminationGracePeriodSeconds: 30
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          volumes:
          - emptyDir: {}
            name: tmp
          - configMap:
              defaultMode: 420
              items:
              - key: Corefile
                path: Corefile
              name: coredns
            name: config-volume
    status:
      availableReplicas: 2
      conditions:
      - lastTransitionTime: "2020-04-27T20:34:50Z"
        lastUpdateTime: "2020-04-27T20:34:50Z"
        message: Deployment has minimum availability.
        reason: MinimumReplicasAvailable
        status: "True"
        type: Available
      - lastTransitionTime: "2020-04-27T20:34:35Z"
        lastUpdateTime: "2020-04-27T20:35:06Z"
        message: ReplicaSet "coredns-5d4dd4b4db" has successfully progressed.
        reason: NewReplicaSetAvailable
        status: "True"
        type: Progressing
      observedGeneration: 1
      readyReplicas: 2
      replicas: 2
      updatedReplicas: 2
  - apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      annotations:
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:35:44Z"
      generation: 2
      labels:
        k8s-app: kubernetes-dashboard
      name: kubernetes-dashboard
      namespace: kube-system
      resourceVersion: "2210"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/kubernetes-dashboard
      uid: e8a80f24-557e-411d-9f55-5c2386bce605
    spec:
      progressDeadlineSeconds: 600
      replicas: 1
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          k8s-app: kubernetes-dashboard
      strategy:
        rollingUpdate:
          maxSurge: 25%
          maxUnavailable: 25%
        type: RollingUpdate
      template:
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kubernetes-dashboard
        spec:
          containers:
          - args:
            - --auto-generate-certificates
            image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 3
              httpGet:
                path: /
                port: 8443
                scheme: HTTPS
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 30
            name: kubernetes-dashboard
            ports:
            - containerPort: 8443
              protocol: TCP
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /certs
              name: kubernetes-dashboard-certs
            - mountPath: /tmp
              name: tmp-volume
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: kubernetes-dashboard
          serviceAccountName: kubernetes-dashboard
          terminationGracePeriodSeconds: 30
          tolerations:
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          volumes:
          - name: kubernetes-dashboard-certs
            secret:
              defaultMode: 420
              secretName: kubernetes-dashboard-certs
          - emptyDir: {}
            name: tmp-volume
    status:
      availableReplicas: 1
      conditions:
      - lastTransitionTime: "2020-04-27T20:35:49Z"
        lastUpdateTime: "2020-04-27T20:35:49Z"
        message: Deployment has minimum availability.
        reason: MinimumReplicasAvailable
        status: "True"
        type: Available
      - lastTransitionTime: "2020-04-27T20:35:44Z"
        lastUpdateTime: "2020-04-27T20:35:49Z"
        message: ReplicaSet "kubernetes-dashboard-7d75c474bb" has successfully progressed.
        reason: NewReplicaSetAvailable
        status: "True"
        type: Progressing
      observedGeneration: 2
      readyReplicas: 1
      replicas: 1
      updatedReplicas: 1
  - apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      annotations:
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:35:47Z"
      generation: 2
      name: namespace-configuration-operator
      namespace: kube-system
      resourceVersion: "2223"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/namespace-configuration-operator
      uid: 90a9a6de-da61-46a1-9d08-174f551e173e
    spec:
      progressDeadlineSeconds: 600
      replicas: 1
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          name: namespace-configuration-operator
      strategy:
        rollingUpdate:
          maxSurge: 25%
          maxUnavailable: 25%
        type: RollingUpdate
      template:
        metadata:
          creationTimestamp: null
          labels:
            name: namespace-configuration-operator
        spec:
          containers:
          - command:
            - namespace-configuration-operator
            env:
            - name: WATCH_NAMESPACE
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: OPERATOR_NAME
              value: namespace-configuration-operator
            image: quay.io/redhat-cop/namespace-configuration-operator:v0.1.0
            imagePullPolicy: IfNotPresent
            name: namespace-configuration-operator
            resources:
              limits:
                cpu: 100m
                memory: 256Mi
              requests:
                cpu: 10m
                memory: 20Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: namespace-configuration-operator
          serviceAccountName: namespace-configuration-operator
          terminationGracePeriodSeconds: 30
    status:
      availableReplicas: 1
      conditions:
      - lastTransitionTime: "2020-04-27T20:35:53Z"
        lastUpdateTime: "2020-04-27T20:35:53Z"
        message: Deployment has minimum availability.
        reason: MinimumReplicasAvailable
        status: "True"
        type: Available
      - lastTransitionTime: "2020-04-27T20:35:47Z"
        lastUpdateTime: "2020-04-27T20:35:53Z"
        message: ReplicaSet "namespace-configuration-operator-6f748b789c" has successfully
          progressed.
        reason: NewReplicaSetAvailable
        status: "True"
        type: Progressing
      observedGeneration: 2
      readyReplicas: 1
      replicas: 1
      updatedReplicas: 1
  - apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      annotations:
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:35:48Z"
      generation: 2
      name: platform-operator
      namespace: kube-system
      resourceVersion: "2233"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/platform-operator
      uid: a60fb529-a6ba-4edb-ac56-220a78ed8dc6
    spec:
      progressDeadlineSeconds: 600
      replicas: 1
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          name: platform-operator
      strategy:
        rollingUpdate:
          maxSurge: 25%
          maxUnavailable: 25%
        type: RollingUpdate
      template:
        metadata:
          creationTimestamp: null
          labels:
            name: platform-operator
        spec:
          containers:
          - image: docker.io/moshloop/platform-operator:0.1
            imagePullPolicy: IfNotPresent
            name: platform-operator
            resources:
              limits:
                cpu: 100m
                memory: 256Mi
              requests:
                cpu: 10m
                memory: 20Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: platform-operator
          serviceAccountName: platform-operator
          terminationGracePeriodSeconds: 30
    status:
      availableReplicas: 1
      conditions:
      - lastTransitionTime: "2020-04-27T20:35:55Z"
        lastUpdateTime: "2020-04-27T20:35:55Z"
        message: Deployment has minimum availability.
        reason: MinimumReplicasAvailable
        status: "True"
        type: Available
      - lastTransitionTime: "2020-04-27T20:35:48Z"
        lastUpdateTime: "2020-04-27T20:35:55Z"
        message: ReplicaSet "platform-operator-995b7fb78" has successfully progressed.
        reason: NewReplicaSetAvailable
        status: "True"
        type: Progressing
      observedGeneration: 2
      readyReplicas: 1
      replicas: 1
      updatedReplicas: 1
  metadata: {}
- items:
  - apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
      annotations:
        kubernetes.io/tls-acme: "true"
        nginx.ingress.kubernetes.io/backend-protocol: HTTPS
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      creationTimestamp: "2020-04-27T20:35:45Z"
      generation: 1
      name: dashboard-ing
      namespace: kube-system
      resourceVersion: "1068"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/ingresses/dashboard-ing
      uid: 96daeb72-df4c-4caa-8faf-b4b895a90219
    spec:
      rules:
      - host: dashboard.127.0.0.1.nip.io
        http:
          paths:
          - backend:
              serviceName: kubernetes-dashboard
              servicePort: 443
      tls:
      - hosts:
        - dashboard.127.0.0.1.nip.io
        secretName: dashboard-tls
    status:
      loadBalancer: {}
  metadata: {}
- items:
  - apiVersion: extensions/v1beta1
    kind: ReplicaSet
    metadata:
      annotations:
        deployment.kubernetes.io/desired-replicas: "1"
        deployment.kubernetes.io/max-replicas: "1"
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:34:35Z"
      generation: 1
      labels:
        k8s-app: calico-kube-controllers
        pod-template-hash: 65b8787765
      name: calico-kube-controllers-65b8787765
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: Deployment
        name: calico-kube-controllers
        uid: 7b3ad329-42ac-4f5c-b982-1a382a91ccca
      resourceVersion: "669"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/calico-kube-controllers-65b8787765
      uid: 8492a565-f513-4f1a-aace-c607bc964343
    spec:
      replicas: 1
      selector:
        matchLabels:
          k8s-app: calico-kube-controllers
          pod-template-hash: 65b8787765
      template:
        metadata:
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ""
          creationTimestamp: null
          labels:
            k8s-app: calico-kube-controllers
            pod-template-hash: 65b8787765
          name: calico-kube-controllers
          namespace: kube-system
        spec:
          containers:
          - env:
            - name: ENABLED_CONTROLLERS
              value: node
            - name: DATASTORE_TYPE
              value: kubernetes
            image: calico/kube-controllers:v3.8.2
            imagePullPolicy: IfNotPresent
            name: calico-kube-controllers
            readinessProbe:
              exec:
                command:
                - /usr/bin/check-status
                - -r
              failureThreshold: 3
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          nodeSelector:
            beta.kubernetes.io/os: linux
          priorityClassName: system-cluster-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: calico-kube-controllers
          serviceAccountName: calico-kube-controllers
          terminationGracePeriodSeconds: 30
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
    status:
      availableReplicas: 1
      fullyLabeledReplicas: 1
      observedGeneration: 1
      readyReplicas: 1
      replicas: 1
  - apiVersion: extensions/v1beta1
    kind: ReplicaSet
    metadata:
      annotations:
        deployment.kubernetes.io/desired-replicas: "2"
        deployment.kubernetes.io/max-replicas: "3"
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:34:35Z"
      generation: 1
      labels:
        k8s-app: kube-dns
        pod-template-hash: 5d4dd4b4db
      name: coredns-5d4dd4b4db
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: Deployment
        name: coredns
        uid: 40ad49e0-7f0a-490c-b7d1-083faf79059f
      resourceVersion: "743"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/coredns-5d4dd4b4db
      uid: 02a4e24b-1af1-4f32-ba89-fbd10e813ca9
    spec:
      replicas: 2
      selector:
        matchLabels:
          k8s-app: kube-dns
          pod-template-hash: 5d4dd4b4db
      template:
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kube-dns
            pod-template-hash: 5d4dd4b4db
        spec:
          containers:
          - args:
            - -conf
            - /etc/coredns/Corefile
            image: k8s.gcr.io/coredns:1.3.1
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 5
              httpGet:
                path: /health
                port: 8080
                scheme: HTTP
              initialDelaySeconds: 60
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 5
            name: coredns
            ports:
            - containerPort: 53
              name: dns
              protocol: UDP
            - containerPort: 53
              name: dns-tcp
              protocol: TCP
            - containerPort: 9153
              name: metrics
              protocol: TCP
            readinessProbe:
              failureThreshold: 3
              httpGet:
                path: /health
                port: 8080
                scheme: HTTP
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            resources:
              limits:
                memory: 170Mi
              requests:
                cpu: 100m
                memory: 70Mi
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                add:
                - NET_BIND_SERVICE
                drop:
                - all
              readOnlyRootFilesystem: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /etc/coredns
              name: config-volume
              readOnly: true
            - mountPath: /tmp
              name: tmp
          dnsPolicy: Default
          nodeSelector:
            beta.kubernetes.io/os: linux
          priorityClassName: system-cluster-critical
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: coredns
          serviceAccountName: coredns
          terminationGracePeriodSeconds: 30
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          volumes:
          - emptyDir: {}
            name: tmp
          - configMap:
              defaultMode: 420
              items:
              - key: Corefile
                path: Corefile
              name: coredns
            name: config-volume
    status:
      availableReplicas: 2
      fullyLabeledReplicas: 2
      observedGeneration: 1
      readyReplicas: 2
      replicas: 2
  - apiVersion: extensions/v1beta1
    kind: ReplicaSet
    metadata:
      annotations:
        deployment.kubernetes.io/desired-replicas: "1"
        deployment.kubernetes.io/max-replicas: "2"
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:35:44Z"
      generation: 1
      labels:
        k8s-app: kubernetes-dashboard
        pod-template-hash: 7d75c474bb
      name: kubernetes-dashboard-7d75c474bb
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: Deployment
        name: kubernetes-dashboard
        uid: e8a80f24-557e-411d-9f55-5c2386bce605
      resourceVersion: "1173"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/kubernetes-dashboard-7d75c474bb
      uid: 693b795b-0fa7-4c83-be3d-e14db10528a6
    spec:
      replicas: 1
      selector:
        matchLabels:
          k8s-app: kubernetes-dashboard
          pod-template-hash: 7d75c474bb
      template:
        metadata:
          creationTimestamp: null
          labels:
            k8s-app: kubernetes-dashboard
            pod-template-hash: 7d75c474bb
        spec:
          containers:
          - args:
            - --auto-generate-certificates
            image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 3
              httpGet:
                path: /
                port: 8443
                scheme: HTTPS
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 30
            name: kubernetes-dashboard
            ports:
            - containerPort: 8443
              protocol: TCP
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /certs
              name: kubernetes-dashboard-certs
            - mountPath: /tmp
              name: tmp-volume
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: kubernetes-dashboard
          serviceAccountName: kubernetes-dashboard
          terminationGracePeriodSeconds: 30
          tolerations:
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          volumes:
          - name: kubernetes-dashboard-certs
            secret:
              defaultMode: 420
              secretName: kubernetes-dashboard-certs
          - emptyDir: {}
            name: tmp-volume
    status:
      availableReplicas: 1
      fullyLabeledReplicas: 1
      observedGeneration: 1
      readyReplicas: 1
      replicas: 1
  - apiVersion: extensions/v1beta1
    kind: ReplicaSet
    metadata:
      annotations:
        deployment.kubernetes.io/desired-replicas: "1"
        deployment.kubernetes.io/max-replicas: "2"
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:35:47Z"
      generation: 1
      labels:
        name: namespace-configuration-operator
        pod-template-hash: 6f748b789c
      name: namespace-configuration-operator-6f748b789c
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: Deployment
        name: namespace-configuration-operator
        uid: 90a9a6de-da61-46a1-9d08-174f551e173e
      resourceVersion: "1269"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/namespace-configuration-operator-6f748b789c
      uid: c2b7e9e4-fd04-4c2b-b1ce-24d16a201646
    spec:
      replicas: 1
      selector:
        matchLabels:
          name: namespace-configuration-operator
          pod-template-hash: 6f748b789c
      template:
        metadata:
          creationTimestamp: null
          labels:
            name: namespace-configuration-operator
            pod-template-hash: 6f748b789c
        spec:
          containers:
          - command:
            - namespace-configuration-operator
            env:
            - name: WATCH_NAMESPACE
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: OPERATOR_NAME
              value: namespace-configuration-operator
            image: quay.io/redhat-cop/namespace-configuration-operator:v0.1.0
            imagePullPolicy: IfNotPresent
            name: namespace-configuration-operator
            resources:
              limits:
                cpu: 100m
                memory: 256Mi
              requests:
                cpu: 10m
                memory: 20Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: namespace-configuration-operator
          serviceAccountName: namespace-configuration-operator
          terminationGracePeriodSeconds: 30
    status:
      availableReplicas: 1
      fullyLabeledReplicas: 1
      observedGeneration: 1
      readyReplicas: 1
      replicas: 1
  - apiVersion: extensions/v1beta1
    kind: ReplicaSet
    metadata:
      annotations:
        deployment.kubernetes.io/desired-replicas: "1"
        deployment.kubernetes.io/max-replicas: "2"
        deployment.kubernetes.io/revision: "1"
      creationTimestamp: "2020-04-27T20:35:48Z"
      generation: 1
      labels:
        name: platform-operator
        pod-template-hash: 995b7fb78
      name: platform-operator-995b7fb78
      namespace: kube-system
      ownerReferences:
      - apiVersion: apps/v1
        blockOwnerDeletion: true
        controller: true
        kind: Deployment
        name: platform-operator
        uid: a60fb529-a6ba-4edb-ac56-220a78ed8dc6
      resourceVersion: "1334"
      selfLink: /apis/extensions/v1beta1/namespaces/kube-system/replicasets/platform-operator-995b7fb78
      uid: 711594c6-dc79-4eaf-a72f-03fb8e1244d4
    spec:
      replicas: 1
      selector:
        matchLabels:
          name: platform-operator
          pod-template-hash: 995b7fb78
      template:
        metadata:
          creationTimestamp: null
          labels:
            name: platform-operator
            pod-template-hash: 995b7fb78
        spec:
          containers:
          - image: docker.io/moshloop/platform-operator:0.1
            imagePullPolicy: IfNotPresent
            name: platform-operator
            resources:
              limits:
                cpu: 100m
                memory: 256Mi
              requests:
                cpu: 10m
                memory: 20Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          serviceAccount: platform-operator
          serviceAccountName: platform-operator
          terminationGracePeriodSeconds: 30
    status:
      availableReplicas: 1
      fullyLabeledReplicas: 1
      observedGeneration: 1
      readyReplicas: 1
      replicas: 1
  metadata: {}
- items:
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: namespace-configuration-operator
      usage:
        cpu: 5m
        memory: 21600Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: namespace-configuration-operator-6f748b789c-p2v6j
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/namespace-configuration-operator-6f748b789c-p2v6j
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: node-cache
      usage:
        cpu: 10m
        memory: 27600Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: node-local-dns-ttl7q
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/node-local-dns-ttl7q
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: kube-controller-manager
      usage:
        cpu: 55m
        memory: 164604Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: kube-controller-manager-kind-control-plane
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/kube-controller-manager-kind-control-plane
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: kube-apiserver
      usage:
        cpu: 271m
        memory: 885376Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: kube-apiserver-kind-control-plane
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/kube-apiserver-kind-control-plane
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: kube-proxy
      usage:
        cpu: 16m
        memory: 36736Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: kube-proxy-cdzt5
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/kube-proxy-cdzt5
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: platform-operator
      usage:
        cpu: "0"
        memory: 10304Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: platform-operator-995b7fb78-9m4pj
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/platform-operator-995b7fb78-9m4pj
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: etcd
      usage:
        cpu: 119m
        memory: 192408Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: etcd-kind-control-plane
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/etcd-kind-control-plane
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: kube-scheduler
      usage:
        cpu: 4m
        memory: 35032Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: kube-scheduler-kind-control-plane
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/kube-scheduler-kind-control-plane
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: kubernetes-dashboard
      usage:
        cpu: "0"
        memory: 23720Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: kubernetes-dashboard-7d75c474bb-ggrrx
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/kubernetes-dashboard-7d75c474bb-ggrrx
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: calico-kube-controllers
      usage:
        cpu: 1m
        memory: 17840Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: calico-kube-controllers-65b8787765-q8mpm
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/calico-kube-controllers-65b8787765-q8mpm
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: calico-node
      usage:
        cpu: 107m
        memory: 72184Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: calico-node-lvdsp
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/calico-node-lvdsp
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: coredns
      usage:
        cpu: 6m
        memory: 26152Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: coredns-5d4dd4b4db-jp62r
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/coredns-5d4dd4b4db-jp62r
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  - apiVersion: metrics.k8s.io/v1beta1
    containers:
    - name: coredns
      usage:
        cpu: 6m
        memory: 24036Ki
    kind: PodMetrics
    metadata:
      creationTimestamp: "2020-04-27T20:46:10Z"
      name: coredns-5d4dd4b4db-7w9bv
      namespace: kube-system
      selfLink: /apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/coredns-5d4dd4b4db-7w9bv
    timestamp: "2020-04-27T20:46:10Z"
    window: 5m0s
  metadata: {}
- items:
  - apiVersion: monitoring.coreos.com/v1
    kind: ServiceMonitor
    metadata:
      creationTimestamp: "2020-04-27T20:40:29Z"
      generation: 1
      labels:
        component: etcd
        tier: control-plane
      name: etcd
      namespace: kube-system
      resourceVersion: "3915"
      selfLink: /apis/monitoring.coreos.com/v1/namespaces/kube-system/servicemonitors/etcd
      uid: 1cbb3065-7c73-4025-b45a-9ab8cb3b02cc
    spec:
      endpoints:
      - interval: 30s
        port: http-metrics
      namespaceSelector:
        matchNames:
        - kube-system
      selector:
        matchLabels:
          component: etcd
          tier: control-plane
  metadata: {}
- items:
  - apiVersion: networking.k8s.io/v1beta1
    kind: Ingress
    metadata:
      annotations:
        kubernetes.io/tls-acme: "true"
        nginx.ingress.kubernetes.io/backend-protocol: HTTPS
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      creationTimestamp: "2020-04-27T20:35:45Z"
      generation: 1
      name: dashboard-ing
      namespace: kube-system
      resourceVersion: "1068"
      selfLink: /apis/networking.k8s.io/v1beta1/namespaces/kube-system/ingresses/dashboard-ing
      uid: 96daeb72-df4c-4caa-8faf-b4b895a90219
    spec:
      rules:
      - host: dashboard.127.0.0.1.nip.io
        http:
          paths:
          - backend:
              serviceName: kubernetes-dashboard
              servicePort: 443
      tls:
      - hosts:
        - dashboard.127.0.0.1.nip.io
        secretName: dashboard-tls
    status:
      loadBalancer: {}
  metadata: {}
- items:
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      creationTimestamp: "2020-04-27T20:34:38Z"
      labels:
        app: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
      name: cert-manager-cainjector:leaderelection
      namespace: kube-system
      resourceVersion: "505"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/cert-manager-cainjector:leaderelection
      uid: 1e5e7314-25fc-43d2-9a85-039cc2e2bd3a
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: cert-manager-cainjector:leaderelection
    subjects:
    - kind: ServiceAccount
      name: cert-manager-cainjector
      namespace: cert-manager
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      creationTimestamp: "2020-04-27T20:34:39Z"
      labels:
        app: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
      name: cert-manager-webhook:webhook-authentication-reader
      namespace: kube-system
      resourceVersion: "508"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/cert-manager-webhook:webhook-authentication-reader
      uid: 142ad66b-4f40-4623-ae5a-8dcae230fcb5
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: extension-apiserver-authentication-reader
    subjects:
    - kind: ServiceAccount
      name: cert-manager-webhook
      namespace: cert-manager
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      creationTimestamp: "2020-04-27T20:34:40Z"
      labels:
        app: cert-manager
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
      name: cert-manager:leaderelection
      namespace: kube-system
      resourceVersion: "517"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/cert-manager:leaderelection
      uid: f3f05c8f-b404-4153-8218-7975fbebd853
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: cert-manager:leaderelection
    subjects:
    - kind: ServiceAccount
      name: cert-manager
      namespace: cert-manager
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      creationTimestamp: "2020-04-27T20:34:17Z"
      name: kube-proxy
      namespace: kube-system
      resourceVersion: "181"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/kube-proxy
      uid: aa4198c9-a674-41a6-8331-52afc41281d7
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: kube-proxy
    subjects:
    - apiGroup: rbac.authorization.k8s.io
      kind: Group
      name: system:bootstrappers:kubeadm:default-node-token
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      creationTimestamp: "2020-04-27T20:34:16Z"
      name: kubeadm:kubelet-config-1.15
      namespace: kube-system
      resourceVersion: "157"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/kubeadm:kubelet-config-1.15
      uid: bb7b7c5c-fecd-4ea4-8547-97861d9a687a
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: kubeadm:kubelet-config-1.15
    subjects:
    - apiGroup: rbac.authorization.k8s.io
      kind: Group
      name: system:nodes
    - apiGroup: rbac.authorization.k8s.io
      kind: Group
      name: system:bootstrappers:kubeadm:default-node-token
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      creationTimestamp: "2020-04-27T20:34:16Z"
      name: kubeadm:nodes-kubeadm-config
      namespace: kube-system
      resourceVersion: "154"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/kubeadm:nodes-kubeadm-config
      uid: a556ffff-ad62-4eed-8ec1-acf3ad57ff04
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: kubeadm:nodes-kubeadm-config
    subjects:
    - apiGroup: rbac.authorization.k8s.io
      kind: Group
      name: system:bootstrappers:kubeadm:default-node-token
    - apiGroup: rbac.authorization.k8s.io
      kind: Group
      name: system:nodes
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      creationTimestamp: "2020-04-27T20:35:44Z"
      name: kubernetes-dashboard-minimal
      namespace: kube-system
      resourceVersion: "1031"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/kubernetes-dashboard-minimal
      uid: 8784a289-8dd1-48f6-9520-13575b977d13
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: kubernetes-dashboard-minimal
    subjects:
    - kind: ServiceAccount
      name: kubernetes-dashboard
      namespace: kube-system
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      creationTimestamp: "2020-04-27T20:35:35Z"
      name: quack:system:extension-apiserver-authentication-reader
      namespace: kube-system
      resourceVersion: "922"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/quack:system:extension-apiserver-authentication-reader
      uid: ff20b521-1bfb-481a-a67c-e30b63ef5889
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: extension-apiserver-authentication-reader
    subjects:
    - kind: ServiceAccount
      name: quack
      namespace: quack
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      creationTimestamp: "2020-04-27T20:40:18Z"
      name: resource-metrics-auth-reader
      namespace: kube-system
      resourceVersion: "3612"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/resource-metrics-auth-reader
      uid: 0704103a-5d05-4b12-9928-dcdc2fab850d
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: extension-apiserver-authentication-reader
    subjects:
    - kind: ServiceAccount
      name: prometheus-adapter
      namespace: monitoring
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      creationTimestamp: "2020-04-27T20:34:16Z"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system::extension-apiserver-authentication-reader
      namespace: kube-system
      resourceVersion: "140"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system::extension-apiserver-authentication-reader
      uid: 1813f230-b761-45c9-9c6f-1e907c7cfa86
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: extension-apiserver-authentication-reader
    subjects:
    - apiGroup: rbac.authorization.k8s.io
      kind: User
      name: system:kube-controller-manager
    - apiGroup: rbac.authorization.k8s.io
      kind: User
      name: system:kube-scheduler
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      creationTimestamp: "2020-04-27T20:34:16Z"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system::leader-locking-kube-controller-manager
      namespace: kube-system
      resourceVersion: "141"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system::leader-locking-kube-controller-manager
      uid: 21802b65-097e-47ed-ab95-800bdcb09d86
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: system::leader-locking-kube-controller-manager
    subjects:
    - apiGroup: rbac.authorization.k8s.io
      kind: User
      name: system:kube-controller-manager
    - kind: ServiceAccount
      name: kube-controller-manager
      namespace: kube-system
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      creationTimestamp: "2020-04-27T20:34:16Z"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system::leader-locking-kube-scheduler
      namespace: kube-system
      resourceVersion: "142"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system::leader-locking-kube-scheduler
      uid: 61279903-d376-4c0b-a33f-7a2a31a64060
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: system::leader-locking-kube-scheduler
    subjects:
    - apiGroup: rbac.authorization.k8s.io
      kind: User
      name: system:kube-scheduler
    - kind: ServiceAccount
      name: kube-scheduler
      namespace: kube-system
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      creationTimestamp: "2020-04-27T20:34:16Z"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system:controller:bootstrap-signer
      namespace: kube-system
      resourceVersion: "143"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:controller:bootstrap-signer
      uid: 8813c7a9-b58c-47cd-b4bc-fb1817e9af2f
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: system:controller:bootstrap-signer
    subjects:
    - kind: ServiceAccount
      name: bootstrap-signer
      namespace: kube-system
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      creationTimestamp: "2020-04-27T20:34:16Z"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system:controller:cloud-provider
      namespace: kube-system
      resourceVersion: "144"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:controller:cloud-provider
      uid: c61e45e3-ac9f-4b82-91bc-5e26d1ded75e
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: system:controller:cloud-provider
    subjects:
    - kind: ServiceAccount
      name: cloud-provider
      namespace: kube-system
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      creationTimestamp: "2020-04-27T20:34:16Z"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system:controller:token-cleaner
      namespace: kube-system
      resourceVersion: "145"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:controller:token-cleaner
      uid: b33b2f9c-d117-40e4-86eb-e6ba1b140ecb
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: system:controller:token-cleaner
    subjects:
    - kind: ServiceAccount
      name: token-cleaner
      namespace: kube-system
  metadata: {}
- items:
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      creationTimestamp: "2020-04-27T20:34:38Z"
      labels:
        app: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
      name: cert-manager-cainjector:leaderelection
      namespace: kube-system
      resourceVersion: "503"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/cert-manager-cainjector:leaderelection
      uid: 556b8c74-6653-4e66-b2ca-0436125dc915
    rules:
    - apiGroups:
      - ""
      resources:
      - configmaps
      verbs:
      - get
      - create
      - update
      - patch
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      creationTimestamp: "2020-04-27T20:34:40Z"
      labels:
        app: cert-manager
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
      name: cert-manager:leaderelection
      namespace: kube-system
      resourceVersion: "513"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/cert-manager:leaderelection
      uid: 4588cd7f-5e24-4ae5-95e7-285c40cf0831
    rules:
    - apiGroups:
      - ""
      resources:
      - configmaps
      verbs:
      - get
      - create
      - update
      - patch
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      creationTimestamp: "2020-04-27T20:34:16Z"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: extension-apiserver-authentication-reader
      namespace: kube-system
      resourceVersion: "133"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/extension-apiserver-authentication-reader
      uid: 020ab6c7-d7cb-4d6a-a50e-be6fb65b05d1
    rules:
    - apiGroups:
      - ""
      resourceNames:
      - extension-apiserver-authentication
      resources:
      - configmaps
      verbs:
      - get
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      creationTimestamp: "2020-04-27T20:34:17Z"
      name: kube-proxy
      namespace: kube-system
      resourceVersion: "180"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/kube-proxy
      uid: c62abb07-7ae8-49c8-a309-511e25846bf1
    rules:
    - apiGroups:
      - ""
      resourceNames:
      - kube-proxy
      resources:
      - configmaps
      verbs:
      - get
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      creationTimestamp: "2020-04-27T20:34:16Z"
      name: kubeadm:kubelet-config-1.15
      namespace: kube-system
      resourceVersion: "156"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/kubeadm:kubelet-config-1.15
      uid: 55bb791f-917d-4a5e-bd10-fc6d5ea781c5
    rules:
    - apiGroups:
      - ""
      resourceNames:
      - kubelet-config-1.15
      resources:
      - configmaps
      verbs:
      - get
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      creationTimestamp: "2020-04-27T20:34:16Z"
      name: kubeadm:nodes-kubeadm-config
      namespace: kube-system
      resourceVersion: "153"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/kubeadm:nodes-kubeadm-config
      uid: e868fd31-431c-4230-b484-77a540b58423
    rules:
    - apiGroups:
      - ""
      resourceNames:
      - kubeadm-config
      resources:
      - configmaps
      verbs:
      - get
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      creationTimestamp: "2020-04-27T20:35:44Z"
      name: kubernetes-dashboard-minimal
      namespace: kube-system
      resourceVersion: "1029"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/kubernetes-dashboard-minimal
      uid: 5098c2db-96f1-4c93-a83b-18adf841f972
    rules:
    - apiGroups:
      - ""
      resources:
      - secrets
      verbs:
      - create
    - apiGroups:
      - ""
      resources:
      - configmaps
      verbs:
      - create
    - apiGroups:
      - ""
      resourceNames:
      - kubernetes-dashboard-key-holder
      - kubernetes-dashboard-certs
      resources:
      - secrets
      verbs:
      - get
      - update
      - delete
    - apiGroups:
      - ""
      resourceNames:
      - kubernetes-dashboard-settings
      resources:
      - configmaps
      verbs:
      - get
      - update
    - apiGroups:
      - ""
      resourceNames:
      - heapster
      resources:
      - services
      verbs:
      - proxy
    - apiGroups:
      - ""
      resourceNames:
      - heapster
      - 'http:heapster:'
      - 'https:heapster:'
      resources:
      - services/proxy
      verbs:
      - get
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      creationTimestamp: "2020-04-27T20:34:16Z"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system::leader-locking-kube-controller-manager
      namespace: kube-system
      resourceVersion: "137"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system::leader-locking-kube-controller-manager
      uid: a98bd7e3-91a9-49bc-8586-aa7e7123391c
    rules:
    - apiGroups:
      - ""
      resources:
      - configmaps
      verbs:
      - watch
    - apiGroups:
      - ""
      resourceNames:
      - kube-controller-manager
      resources:
      - configmaps
      verbs:
      - get
      - update
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      creationTimestamp: "2020-04-27T20:34:16Z"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system::leader-locking-kube-scheduler
      namespace: kube-system
      resourceVersion: "138"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system::leader-locking-kube-scheduler
      uid: 52ca5477-dfdb-47e1-a6f5-0c89e37a1522
    rules:
    - apiGroups:
      - ""
      resources:
      - configmaps
      verbs:
      - watch
    - apiGroups:
      - ""
      resourceNames:
      - kube-scheduler
      resources:
      - configmaps
      verbs:
      - get
      - update
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      creationTimestamp: "2020-04-27T20:34:16Z"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system:controller:bootstrap-signer
      namespace: kube-system
      resourceVersion: "134"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system:controller:bootstrap-signer
      uid: 5d135182-03a5-4b9f-b49c-9fec354bb8bb
    rules:
    - apiGroups:
      - ""
      resources:
      - secrets
      verbs:
      - get
      - list
      - watch
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      creationTimestamp: "2020-04-27T20:34:16Z"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system:controller:cloud-provider
      namespace: kube-system
      resourceVersion: "135"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system:controller:cloud-provider
      uid: 1a3f3d63-efcf-43d7-b69d-7e8016af72b6
    rules:
    - apiGroups:
      - ""
      resources:
      - configmaps
      verbs:
      - create
      - get
      - list
      - watch
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      creationTimestamp: "2020-04-27T20:34:16Z"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system:controller:token-cleaner
      namespace: kube-system
      resourceVersion: "136"
      selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system:controller:token-cleaner
      uid: deb52b55-6ebe-479b-8eb9-e7041ffa9bfb
    rules:
    - apiGroups:
      - ""
      resources:
      - secrets
      verbs:
      - delete
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - events
      verbs:
      - create
      - patch
      - update
  metadata: {}
kind: List
metadata: {}
